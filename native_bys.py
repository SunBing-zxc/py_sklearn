import streamlit as st
import numpy as np
import pandas as pd
from collections import defaultdict
import re
import jieba
from sklearn.model_selection import train_test_split  # å¯¼å…¥æ•°æ®é›†åˆ’åˆ†å‡½æ•°
from sklearn.metrics import accuracy_score, confusion_matrix

# å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæ‰©å¤§æ ·æœ¬é‡ï¼‰
def prepare_training_data():
    # æ­£å¸¸çŸ­ä¿¡æ ·æœ¬
    normal_sms = [
        "æ‚¨çš„è¯è´¹ä½™é¢ä¸è¶³10å…ƒï¼Œè¯·åŠæ—¶å……å€¼ï¼Œé¿å…åœæœºå½±å“ä½¿ç”¨",
        "ã€å¿«é€’é€šçŸ¥ã€‘æ‚¨çš„åŒ…è£¹å·²åˆ°è¾¾å°åŒºé©¿ç«™ï¼Œå–ä»¶ç 123456ï¼Œæœ‰æ•ˆæœŸ2å¤©",
        "æœ¬å‘¨å…­ä¸‹åˆ3ç‚¹åŒå­¦èšä¼šï¼Œåœ°ç‚¹åœ¨å­¦æ ¡é—¨å£é¤å…ï¼Œæ”¶åˆ°è¯·å›å¤",
        "ç”µè´¹æé†’ï¼šæ‚¨å®¶7æœˆç”µè´¹150.50å…ƒï¼Œæˆªæ­¢æ—¥æœŸ8æœˆ10æ—¥ï¼Œå¯é€šè¿‡APPç¼´çº³",
        "å¤©æ°”é¢„æŠ¥ï¼šæ˜å¤©å¤šäº‘è½¬å°é›¨ï¼Œæ°”æ¸©24-30â„ƒï¼Œè®°å¾—å¸¦ä¼",
        "ã€é“¶è¡Œã€‘æ‚¨çš„å‚¨è“„å¡è´¦æˆ·äº08:30å­˜å…¥å·¥èµ„5000å…ƒï¼Œä½™é¢12500å…ƒ",
        "å®¶é•¿ä¼šé€šçŸ¥ï¼šæœ¬å‘¨äº”ä¸‹åˆ4ç‚¹åœ¨æ•™å®¤å¬å¼€ï¼Œè¯·å‹¿è¿Ÿåˆ°",
        "æ‚¨è®¢è´­çš„å›¾ä¹¦å·²å‘è´§ï¼Œå¿«é€’å•å·SF1234567890",
        "æé†’ï¼šæ˜å¤©æ˜¯æ‚¨çš„ç”Ÿæ—¥ï¼Œç¥æ‚¨ç”Ÿæ—¥å¿«ä¹ï¼",
        "ã€å¤–å–ã€‘æ‚¨ç‚¹çš„é¤å“å·²ç”±éª‘æ‰‹æå¸ˆå‚…æ¥å•ï¼Œé¢„è®¡30åˆ†é’Ÿé€è¾¾",
        "æ‚¨çš„ä¿¡ç”¨å¡è´¦å•å·²å‡ºï¼Œæœ¬æœŸåº”è¿˜é‡‘é¢2350å…ƒï¼Œè¿˜æ¬¾æ—¥9æœˆ5æ—¥",
        "å°åŒºé€šçŸ¥ï¼šæ˜å¤©ä¸Šåˆ9ç‚¹å°†åœæ°´æ£€ä¿®ï¼Œé¢„è®¡3å°æ—¶",
        "å¼ è€å¸ˆï¼Œæˆ‘æ˜¯å­¦ç”Ÿå®¶é•¿ææ¶›ï¼Œå­©å­æ ¡æœå°ºå¯¸é€‰é”™äº†ï¼Œéº»çƒ¦åŠ æˆ‘å¾®ä¿¡ 138xxxx5678 å‘ä¸‹æ­£ç¡®å°ºç è¡¨ï¼Œç€æ€¥ä¸‹å‘¨ç»Ÿä¸€è°ƒæ¢",
        "æ‚¨çš„ä¼šå‘˜ç§¯åˆ†å³å°†åˆ°æœŸï¼Œå¯å…‘æ¢ç¤¼å“æˆ–æŠµæ‰£ç°é‡‘",
        "ã€äº¤é€šè¿ç« ã€‘æ‚¨çš„è½¦è¾†äºXXè·¯æœ‰ä¸€æ¬¡è¿ç« åœè½¦è®°å½•ï¼Œå¯ç½‘ä¸Šå¤„ç†",
        "å…¬å¸é€šçŸ¥ï¼šä¸‹å‘¨ä¸€ä¸Šåˆ10ç‚¹å¬å¼€å…¨ä½“å‘˜å·¥å¤§ä¼šï¼Œè¯·å‡†æ—¶å‚åŠ ",
        "ã€é“¶è¡Œã€‘æ‚¨å°¾å· 3456 çš„å‚¨è“„å¡äº 15:23 æ”¯å‡º 2000 å…ƒï¼ˆä»£ç¼´ç‰©ä¸šè´¹ï¼‰ï¼Œå¦‚æœ‰ç–‘é—®è¯·æ‹¨æ‰“ 955XX",
        "è¯·ç‚¹å‡»é“¾æ¥https://work.weixin.qq.com/s/xxx å¡«å†™æœ¬å‘¨éƒ¨é—¨å›¢å»ºæŠ¥åä¿¡æ¯ï¼Œæˆªæ­¢ä»Šå¤© 18 ç‚¹ ",
    ]
    
    # è¯ˆéª—çŸ­ä¿¡æ ·æœ¬
    fraud_sms = [
        "æ­å–œæ‚¨è·å¾—ä¸€å°ç¬”è®°æœ¬ç”µè„‘ï¼Œå¡«å†™æ”¶è´§åœ°å€å³å¯å…è´¹é¢†å–ï¼Œé™ä»Šæ—¥",        
        "æ­å–œæ‚¨ä¸­äº†äºŒç­‰å¥–5ä¸‡å…ƒï¼è¯·æä¾›é“¶è¡Œå¡å·å’Œèº«ä»½è¯å·é¢†å–ï¼ˆå…‘å¥–ç ï¼š68XXï¼‰",
        "æˆ‘æ˜¯æ‚¨é¢†å¯¼ç‹æ€»ï¼Œæ˜å¤©åˆ°æˆ‘åŠå…¬å®¤ä¸€è¶Ÿï¼Œæœ‰ç¬”ç´§æ€¥æ¬¾é¡¹éœ€è¦ä½ å¸®å¿™å‘¨è½¬",
        "æ‚¨çš„å¿«é€’ä¸¢å¤±ï¼Œç‚¹å‡»ç†èµ”é“¾æ¥å¡«å†™ä¿¡æ¯å³å¯è·èµ”200å…ƒï¼Œ24å°æ—¶å†…æœ‰æ•ˆ",
        "å…è´¹é¢†å–500å…ƒæ‰‹æœºè¯è´¹ï¼å›å¤1å³å¯åŠç†ï¼Œä»…é™ä»Šæ—¥ï¼Œå…ˆåˆ°å…ˆå¾—",
        "ã€æ³•é™¢é€šçŸ¥ã€‘æ‚¨æœ‰ä¸€å¼ ä¼ ç¥¨æœªé¢†å–ï¼Œè¯·ç«‹å³è”ç³»010-12345678æ ¸å®",
        "ä½æ¯è´·æ¬¾ï¼Œæ— æŠµæŠ¼ï¼Œç§’æ‰¹åˆ°è´¦ï¼Œæœ€é«˜50ä¸‡ï¼Œç‚¹å‡»é“¾æ¥å¿«é€ŸåŠç†",
        "æ‚¨çš„å­©å­åœ¨å­¦æ ¡çªå‘ç–¾ç—…ï¼Œæ­£åœ¨åŒ»é™¢æŠ¢æ•‘ï¼Œæ€¥éœ€ç¼´çº³æ‰‹æœ¯è´¹ï¼Œé€Ÿè½¬5ä¸‡å…ƒåˆ°è´¦æˆ·XXX",
        "å…¼èŒåˆ·å•ï¼Œæ—¥å…¥300-500å…ƒï¼Œæ— éœ€æŠ¼é‡‘ï¼Œæ‰«ç åŠ å®¢æœäº†è§£è¯¦æƒ…",
        "ã€ç§»åŠ¨å®¢æœã€‘æ‚¨æœ¬æœˆæ¶ˆè´¹è¾¾æ ‡ï¼Œå¯å…è´¹é¢†å– 20G æµé‡åŒ…ï¼Œç‚¹å‡»é“¾æ¥https://10086-verify.cn éªŒè¯é¢†å–ï¼ˆ1 å°æ—¶å†…æœ‰æ•ˆï¼‰",
        "ã€ç³»ç»Ÿæç¤ºã€‘æ‚¨çš„å¾®ä¿¡è´¦å·å­˜åœ¨å®‰å…¨é£é™©ï¼Œç‚¹å‡»é“¾æ¥å®Œæˆå®åè®¤è¯",
        "æˆ‘æ˜¯ä½ æœ‹å‹ï¼Œæˆ‘åœ¨å¤–åœ°å‡ºå·®é‡æ€¥äº‹ï¼Œæ€¥éœ€ç”¨é’±ï¼Œå…ˆè½¬2ä¸‡å…ƒåˆ°è¿™ä¸ªè´¦æˆ·",
        "æ‚¨è¢«é€‰ä¸ºå¹¸è¿ç”¨æˆ·ï¼Œå¯å…è´¹é¢†å–ä¸€å°æ™ºèƒ½æ‰‹æœºï¼Œåªéœ€æ”¯ä»˜29å…ƒè¿è´¹",
        "æ£€æµ‹åˆ°æ‚¨çš„ç¤¾ä¿è´¦æˆ·æœªå¹´å®¡ï¼Œé€¾æœŸå°†åœç”¨ï¼Œç‚¹å‡»é“¾æ¥åŠç†",
        "æ‚¨åœ¨äº¬ä¸œè´­ä¹°çš„è¿åŠ¨é‹è´¨æ£€æ—¶å‘ç°è½»å¾®ç‘•ç–µï¼Œå¯è¡¥å¿ 50 å…ƒæ— é—¨æ§›åˆ¸ï¼Œå›å¤ã€åŒæ„ã€‘é¢†å–ï¼Œå®¢æœå°†åŒæ­¥åˆ¸ç ",
        "é«˜é¢ä¿¡ç”¨å¡å¿«é€ŸåŠç†ï¼Œæ— éœ€å¾ä¿¡ï¼Œé¢åº¦5-50ä¸‡ï¼Œè”ç³»ç”µè¯138xxxx8888",
        "ã€ç´§æ€¥é€šçŸ¥ã€‘æ‚¨çš„è´¦æˆ·å­˜åœ¨å¼‚å¸¸ï¼Œç‚¹å‡»é“¾æ¥https://xxxéªŒè¯èº«ä»½ï¼Œå¦åˆ™å°†å†»ç»“è´¦æˆ·",
        "æ‚¨çš„å¿«é€’å› åœ°å€æ¨¡ç³Šæ— æ³•æ´¾é€ï¼Œè”ç³»æ´¾ä»¶å‘˜ 135xxxx9012 æˆ–ç‚¹å‡»https://kd100.com/xxx è¡¥å……ä¿¡æ¯ ",
    ]
    
    # æ•´ç†æˆå¸¦æ ‡ç­¾çš„æ•°æ®é›†
    train_data = []
    for sms in normal_sms:
        train_data.append({"text": sms, "label": 0})  # 0è¡¨ç¤ºæ­£å¸¸
    for sms in fraud_sms:
        train_data.append({"text": sms, "label": 1})  # 1è¡¨ç¤ºè¯ˆéª—
    
    return train_data, normal_sms, fraud_sms

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€åˆ†è¯"""
    # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    # åˆ†è¯
    words = jieba.cut(text)
    return " ".join(words)

# è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨è®­ç»ƒé›†ï¼‰
def train_model(train_set):
    # 1. ç»Ÿè®¡å…ˆéªŒæ¦‚ç‡ P(C)
    total_sms = len(train_set)
    p0 = sum(1 for d in train_set if d["label"] == 0) / total_sms  # æ­£å¸¸çŸ­ä¿¡å…ˆéªŒæ¦‚ç‡
    p1 = sum(1 for d in train_set if d["label"] == 1) / total_sms  # è¯ˆéª—çŸ­ä¿¡å…ˆéªŒæ¦‚ç‡
    
    # 2. ç»Ÿè®¡æ¯ä¸ªç±»åˆ«ä¸‹çš„è¯é¢‘ï¼ˆåŠ æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰
    word_count_0 = defaultdict(int)  # æ­£å¸¸çŸ­ä¿¡è¯é¢‘
    word_count_1 = defaultdict(int)  # è¯ˆéª—çŸ­ä¿¡è¯é¢‘
    total_words_0 = 0  # æ­£å¸¸çŸ­ä¿¡æ€»è¯æ•°
    total_words_1 = 0  # è¯ˆéª—çŸ­ä¿¡æ€»è¯æ•°
    
    # åˆ†è¯+ç»Ÿè®¡
    for data in train_set:
        processed_text = preprocess_text(data["text"])
        words = processed_text.split()
        if data["label"] == 0:
            for word in words:
                word_count_0[word] += 1
                total_words_0 += 1
        else:
            for word in words:
                word_count_1[word] += 1
                total_words_1 += 1
    
    # æ‰€æœ‰å”¯ä¸€è¯æ±‡ï¼ˆç”¨äºå¹³æ»‘ï¼‰
    all_words = set(list(word_count_0.keys()) + list(word_count_1.keys()))
    vocab_size = len(all_words)
    alpha = 1  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°
    
    return {
        "p0": p0,
        "p1": p1,
        "word_count_0": word_count_0,
        "word_count_1": word_count_1,
        "total_words_0": total_words_0,
        "total_words_1": total_words_1,
        "vocab_size": vocab_size,
        "alpha": alpha
    }

# é¢„æµ‹å‡½æ•°
def predict_sms(model_params, text):
    """é¢„æµ‹çŸ­ä¿¡æ˜¯å¦ä¸ºè¯ˆéª—"""
    processed_text = preprocess_text(text)
    words = processed_text.split()
    
    p0 = model_params["p0"]
    p1 = model_params["p1"]
    word_count_0 = model_params["word_count_0"]
    word_count_1 = model_params["word_count_1"]
    total_words_0 = model_params["total_words_0"]
    total_words_1 = model_params["total_words_1"]
    vocab_size = model_params["vocab_size"]
    alpha = model_params["alpha"]
    
    # è®¡ç®— P(X|C0)ï¼šæ­£å¸¸çŸ­ä¿¡ä¸‹å‡ºç°è¿™äº›è¯çš„æ¦‚ç‡
    p_x_c0 = 1.0
    for word in words:
        count = word_count_0.get(word, 0)
        p_word_c0 = (count + alpha) / (total_words_0 + alpha * vocab_size)
        p_x_c0 *= p_word_c0
    
    # è®¡ç®— P(X|C1)ï¼šè¯ˆéª—çŸ­ä¿¡ä¸‹å‡ºç°è¿™äº›è¯çš„æ¦‚ç‡
    p_x_c1 = 1.0
    for word in words:
        count = word_count_1.get(word, 0)
        p_word_c1 = (count + alpha) / (total_words_1 + alpha * vocab_size)
        p_x_c1 *= p_word_c1
    
    # åéªŒæ¦‚ç‡ï¼ˆå¿½ç•¥P(X)ï¼Œç›´æ¥æ¯”åˆ†å­ï¼‰
    p_c0_x = p_x_c0 * p0
    p_c1_x = p_x_c1 * p1
    
    # å½’ä¸€åŒ–
    total = p_c0_x + p_c1_x
    p_c0_x_norm = p_c0_x / total if total != 0 else 0
    p_c1_x_norm = p_c1_x / total if total != 0 else 0
    
    # é¢„æµ‹ç±»åˆ«
    pred_label = 0 if p_c0_x > p_c1_x else 1
    pred_label_name = "æ­£å¸¸çŸ­ä¿¡" if pred_label == 0 else "è¯ˆéª—çŸ­ä¿¡"
    
    return {
        "pred_label": pred_label,
        "pred_label_name": pred_label_name,
        "p_normal": p_c0_x_norm,
        "p_fraud": p_c1_x_norm,
        "p_x_c0": p_x_c0,
        "p_x_c1": p_x_c1,
        "processed_text": processed_text
    }

# å±•ç¤ºè®­ç»ƒæ•°æ®ç»Ÿè®¡
def show_data_statistics(normal_sms, fraud_sms, train_size, test_size):
    st.subheader('ğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡')
    
    col1, col2 = st.columns([2,3])
    with col1:
        st.markdown(f"### æ•°æ®é›†åˆ’åˆ†")
        st.write(f"- **æ€»æ ·æœ¬é‡**ï¼š{len(normal_sms) + len(fraud_sms)}æ¡")
        st.write(f"- **æ­£å¸¸çŸ­ä¿¡**ï¼š{len(normal_sms)}æ¡    **è¯ˆéª—çŸ­ä¿¡**ï¼š{len(fraud_sms)}æ¡")
        st.write(f"- **è®­ç»ƒé›†**ï¼š{train_size}æ¡ï¼ˆ80%ï¼‰    **æµ‹è¯•é›†**ï¼š{test_size}æ¡ï¼ˆ20%ï¼‰")
   
    with col2:
        st.markdown("### æ ·æœ¬ç¤ºä¾‹")
        st.info(f"""âœ”ï¸æ­£å¸¸çŸ­ä¿¡ç¤ºä¾‹ï¼š{normal_sms[0]}
            """)
        st.success(f"""âŒè¯ˆéª—çŸ­ä¿¡ç¤ºä¾‹ï¼š{fraud_sms[0]}
            """)


# å±•ç¤ºå…³é”®è¯åˆ†æ
def show_keyword_analysis(model_params):
    st.subheader('ğŸ”‘ å…³é”®è¯åˆ†æ')
    
    # è·å–é«˜é¢‘è¯
    normal_top_words = sorted(
        model_params["word_count_0"].items(), 
        key=lambda x: x[1], 
        reverse=True
    )[:10]
    
    fraud_top_words = sorted(
        model_params["word_count_1"].items(), 
        key=lambda x: x[1], 
        reverse=True
    )[:10]
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("### æ­£å¸¸çŸ­ä¿¡é«˜é¢‘è¯")
        normal_df = pd.DataFrame(normal_top_words, columns=["è¯è¯­", "å‡ºç°æ¬¡æ•°"])
        st.dataframe(normal_df, use_container_width=True)

    
    with col2:
        st.markdown("### è¯ˆéª—çŸ­ä¿¡é«˜é¢‘è¯")
        fraud_df = pd.DataFrame(fraud_top_words, columns=["è¯è¯­", "å‡ºç°æ¬¡æ•°"])
        st.dataframe(fraud_df, use_container_width=True)


# ä¸»äº¤äº’ç•Œé¢
def main_interface(model_params):
    st.markdown('#### ğŸ” çŸ­ä¿¡æ£€æµ‹å·¥å…·')
    
    # é¢„è®¾çŸ­ä¿¡é€‰é¡¹
    preset_sms = [
"ã€è¶…å¸‚é€šçŸ¥ã€‘æ‚¨ä¸Šå‘¨è´­ä¹°çš„æ—¥ç”¨å“å·²å‚ä¸æ»¡å‡æ´»åŠ¨ï¼Œé€€æ¬¾ 25 å…ƒå°†åœ¨ 3 ä¸ªå·¥ä½œæ—¥å†…é€€å›åŸæ”¯ä»˜è´¦æˆ·ï¼Œè¯·æ³¨æ„æŸ¥æ”¶",
"æ‚¨æœ‰ä¸€ä»½æœªé¢†å–çš„å‘¨å¹´åº†ç¤¼å“ï¼Œå†…å«ä»·å€¼ 500 å…ƒè´­ç‰©å¡ï¼Œç‚¹å‡»é“¾æ¥https://gift888.comå¡«å†™åœ°å€å³å¯å…è´¹é¢†å–ï¼Œ24 å°æ—¶å†…æœ‰æ•ˆ",
"å­©å­å­¦æ ¡ç»„ç»‡å‘¨æœ«ç ”å­¦æ´»åŠ¨ï¼Œè´¹ç”¨ 180 å…ƒ / äººï¼Œéœ€åœ¨å‘¨äº”å‰é€šè¿‡å­¦æ ¡å…¬ä¼—å·ç¼´è´¹ï¼Œè¯¦æƒ…å·²å‘è‡³ç­çº§ç¾¤"
    ]
    
    col1, col2 = st.columns([3, 1])
    with col1:
        selected_sms = st.selectbox("é€‰æ‹©é¢„è®¾çŸ­ä¿¡", preset_sms)
    with col2:
        user_guess = st.radio("ä½ çš„åˆ¤æ–­", ["æ­£å¸¸çŸ­ä¿¡", "è¯ˆéª—çŸ­ä¿¡"], key="user_guess",horizontal=True)

       
    if st.button("å¼€å§‹æ£€æµ‹"):
        # æ‰§è¡Œé¢„æµ‹
        result = predict_sms(model_params, selected_sms)                
        st.subheader("æ£€æµ‹ç»“æœ")
           
        # å±•ç¤ºç»“æœå¯¹æ¯”
        col1, col2, col3,col4 = st.columns([1,1,0.1,1])
        with col1:
            st.info(f"ä½ çš„åˆ¤æ–­ï¼š{user_guess}")
        with col2:
            if result["pred_label"] == 1:
                st.error(f"ç®—æ³•åˆ¤æ–­ï¼š{result['pred_label_name']}")
            else:
                st.success(f"ç®—æ³•åˆ¤æ–­ï¼š{result['pred_label_name']}")
        with col4:
            # ç»“æœå¯¹æ¯”
            if user_guess == result['pred_label_name']:
                st.warning("âœ… æ­å–œï¼ä½ çš„åˆ¤æ–­ä¸ç®—æ³•ä¸€è‡´ï½")
            else:
                st.warning("âŒ ä½ çš„åˆ¤æ–­ä¸ç®—æ³•ä¸ä¸€è‡´")
            
        # å±•ç¤ºæ¦‚ç‡
        prob_data = {
            "ç±»å‹": ["æ­£å¸¸çŸ­ä¿¡", "è¯ˆéª—çŸ­ä¿¡"],
            "æ¦‚ç‡": [result["p_normal"], result["p_fraud"]]
        }
        cols=st.columns([2,1])
        with cols[0]:
            # ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºæ•°æ®
            st.subheader("æ¦‚ç‡åˆ†å¸ƒè¡¨")
            st.dataframe(
                prob_data,
                use_container_width=True
            )
    return user_guess

# æ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
def evaluate_model(model_params, test_set):
    st.subheader('ğŸ“ˆ æ¨¡å‹è¯„ä¼°ï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰')
    
    # æµ‹è¯•æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
    y_true = [d["label"] for d in test_set]
    y_pred = [predict_sms(model_params, d["text"])["pred_label"] for d in test_set]
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(y_true, y_pred)
    st.write(f"### æ¨¡å‹å‡†ç¡®ç‡ï¼š{accuracy:.2%}")
    
    cm = confusion_matrix(y_true, y_pred)
    
    # ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
    cols=st.columns([2,1])
    with cols[0]:
        st.markdown("#### ğŸ” æ··æ·†çŸ©é˜µ")
        cm_df = pd.DataFrame(
            cm,
            columns=["é¢„æµ‹ä¸ºæ­£å¸¸çŸ­ä¿¡", "é¢„æµ‹ä¸ºè¯ˆéª—çŸ­ä¿¡"],
            index=["å®é™…ä¸ºæ­£å¸¸çŸ­ä¿¡", "å®é™…ä¸ºè¯ˆéª—çŸ­ä¿¡"]
        )
        st.dataframe(cm_df, use_container_width=True)
    
