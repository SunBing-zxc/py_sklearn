#  C:\Users\å­™å†°\Desktop\AIåŠ©æ•™
#  streamlit run text_analysis_demo.py

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (classification_report, confusion_matrix, 
                             accuracy_score, ConfusionMatrixDisplay)
import os
import jieba
import re
import time
import native_bys
import json
import bayes_text_classification_step_by_step
from api_deepseek import ask_ai_assistant
from datetime import datetime
from learning_report import generate_evaluation
# é¡µé¢è®¾ç½®
st.set_page_config(page_title="æ–‡æœ¬åˆ†æä¸åˆ†ç±»å­¦ä¹ å¹³å°", layout="wide")
st.title("ğŸ“„ æ–‡æœ¬åˆ†æä¸åˆ†ç±»äº¤äº’å¼å­¦ä¹ å¹³å°")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ï¼ˆåœ¨ä¸»ç¨‹åºå…¥å£å¤„ï¼‰
def init_session_state():
    if "text_analysis_records" not in st.session_state:
        st.session_state.text_analysis_records = {
            "text_introduction_section": [],  # æ–‡æœ¬åˆ†æåŸºç¡€
            "text_preprocessing_section": [],  # æ–‡æœ¬é¢„å¤„ç†
            "text_analysis_section": [],  # æ–‡æœ¬åˆ†ç±»ä¸“é¡¹
            "sentiment_analysis_section": [],  # æƒ…æ„Ÿåˆ†æä¸“é¡¹
            "native_bys_section":[], #æœ´ç´ è´å¶æ–¯
            "module_sequence": [],  # æ¨¡å—è®¿é—®é¡ºåº
            "module_timestamps": {},  # æ¨¡å—åœç•™æ—¶é—´
            "text_analysis_quiz": {},  # æµ‹éªŒè®°å½•
            "ai_interactions": []  # AIäº¤äº’è®°å½•
        }

def display_chat_interface(context=""):
    """æ˜¾ç¤ºè´å¶æ–¯æ–‡æœ¬åˆ†ç±»ç›¸å…³çš„èŠå¤©ç•Œé¢"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ’¬ AIåŠ©æ•™å·²å°±ç»ª")
    
    # é¢„è®¾è´å¶æ–¯æ–‡æœ¬åˆ†ç±»ç›¸å…³çš„å¿«æ·é—®é¢˜
    st.sidebar.markdown("**å¿«æ·é—®é¢˜:**")
    col1, col2 = st.sidebar.columns(2)
    
    with col1:
        btn1 = st.button("ä»€ä¹ˆæ˜¯è´å¶æ–¯æ–‡æœ¬åˆ†ç±»?")
        btn2 = st.button("è´å¶æ–¯åˆ†ç±»çš„æ ¸å¿ƒåŸç†?")
    
    with col2:
        btn3 = st.button("TF-IDFçš„ä½œç”¨æ˜¯ä»€ä¹ˆ?")
        btn4 = st.button("è´å¶æ–¯åˆ†ç±»çš„ä¼˜ç¼ºç‚¹?")
    
    # å¤„ç†å¿«æ·é—®é¢˜
    question = ""
    if btn1:
        question = "ä»€ä¹ˆæ˜¯è´å¶æ–¯æ–‡æœ¬åˆ†ç±»?å®ƒé€‚ç”¨äºå“ªäº›åœºæ™¯?"
    elif btn2:
        question = "è´å¶æ–¯æ–‡æœ¬åˆ†ç±»çš„æ ¸å¿ƒåŸç†æ˜¯ä»€ä¹ˆ?åŸºäºå“ªäº›æ•°å­¦å…¬å¼?"
    elif btn3:
        question = "åœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼ŒTF-IDFç‰¹å¾æå–çš„ä½œç”¨æ˜¯ä»€ä¹ˆ?å¦‚ä½•è®¡ç®—?"
    elif btn4:
        question = "è´å¶æ–¯æ–‡æœ¬åˆ†ç±»æœ‰å“ªäº›ä¼˜ç‚¹å’Œç¼ºç‚¹?ä¸å…¶ä»–åˆ†ç±»ç®—æ³•ç›¸æ¯”æœ‰ä½•ç‰¹ç‚¹?"
    
    # æé—®è¾“å…¥æ¡†
    user_input = st.sidebar.text_input("è¾“å…¥ä½ çš„é—®é¢˜(å…³äºè´å¶æ–¯æ–‡æœ¬åˆ†ç±»):", key="question_input")
    if user_input:
        question = user_input
    
    # å¤„ç†æé—®
    if question:

        # è®°å½•AIäº¤äº’
        if "ai_interactions" not in st.session_state.text_analysis_records:
            st.session_state.text_analysis_records["ai_interactions"] = []

        st.session_state.text_analysis_records["ai_interactions"].append({
            "question": question,
            "timestamp": datetime.now().timestamp()
        })
        
        # æ˜¾ç¤ºå½“å‰é—®é¢˜
        st.sidebar.markdown(f"**ä½ :** {question}")
        
        # è·å–å›ç­”
        with st.spinner("åŠ©æ•™æ€è€ƒä¸­..."):
            answer = ask_ai_assistant(question, context)
        
        # æ˜¾ç¤ºå½“å‰å›ç­”
        st.sidebar.markdown(f"**åŠ©æ•™:** {answer}")
        st.sidebar.markdown("---")

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text, is_chinese=False):
    """æ–‡æœ¬é¢„å¤„ç†ï¼šæ¸…æ´—ã€åˆ†è¯"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
    text = re.sub(r'[^a-zA-Z\u4e00-\u9fa5]', ' ', text)
    # è½¬ä¸ºå°å†™
    text = text.lower() if not is_chinese else text
    # åˆ†è¯
    if is_chinese:
        words = jieba.cut(text)
        return " ".join(words)
    else:
        return text

# åŠ è½½ç¤ºä¾‹æ•°æ®
@st.cache_data
def load_sample_data(dataset_name):
    """åŠ è½½ä¸åŒç±»å‹çš„æ–‡æœ¬æ•°æ®é›†"""
    if dataset_name == "æ–°é—»ä¸»é¢˜åˆ†ç±»":
        data_path = os.path.join(
            os.path.dirname(__file__), 
            "datasets", 
            "20newsgroups_selected.json"  # ğŸ‘ˆ ä¿®æ”¹ä¸ºæ–°çš„JSONæ–‡ä»¶å
        )
        
        # 2. è¯»å–æœ¬åœ° JSON æ–‡ä»¶
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{data_path}")
        
        with open(data_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        texts = [preprocess_text(text) for text in dataset["train"]["data"]]
        labels = dataset["train"]["target"]
        label_names = dataset["train"]["target_names"]  # è‹±æ–‡ç±»åˆ«å
        # æ–°å¢ï¼šè·å–ä¸­æ–‡ç±»åˆ«åï¼ˆä»JSONä¸­è¯»å–ï¼‰
        chinese_label_names = dataset["train"]["chinese_target_names"]
        
        # 4. ä¿æŒè¿”å›å€¼å’ŒåŸä»£ç ä¸€è‡´
        return texts, labels, label_names, "è‹±æ–‡"

    
    elif dataset_name == "ä¸­æ–‡æƒ…æ„Ÿåˆ†æ":
        # ä¼˜åŒ–åçš„æ­£é¢æ ·æœ¬ï¼ˆ15æ¡ç‹¬ç«‹æ ·æœ¬ï¼Œå«æ¨¡ç³Šè¡¨è¾¾å’Œå¤æ‚è¯­å¢ƒï¼‰
        positive_samples = [
            "è¿™æ‰‹æœºç»­èˆªæ¯”é¢„æœŸå¥½ï¼Œé‡åº¦ç”¨ä¸€å¤©è¿˜å‰©30%ç”µï¼Œæ€§ä»·æ¯”å¯ä»¥",
            "è™½ç„¶å‘è´§æ…¢äº†ä¸¤å¤©ï¼Œä½†åŒ…è£…å¾ˆç”¨å¿ƒï¼Œäº§å“æ²¡ç‘•ç–µï¼Œæ»¡æ„",
            "å®¢æœæ€åº¦è¶…èµï¼Œè€å¿ƒè§£ç­”äº†æˆ‘ä¸€å †é—®é¢˜ï¼Œå¿…é¡»å¥½è¯„",
            "å‘³é“ä¸ç®—æƒŠè‰³ï¼Œä½†å®¶å¸¸å‘³å¾ˆè¶³ï¼Œåƒç€èˆ’æœï¼Œä¼šå›è´­",
            "å¤–è§‚è®¾è®¡ç®€çº¦å¤§æ°”ï¼Œæ‰‹æ„Ÿæ¯”å›¾ç‰‡çœ‹ç€å¥½ï¼Œå€¼å¾—å…¥æ‰‹",
            "ç¬¬ä¸€æ¬¡ç”¨è¿™ä¸ªç‰Œå­ï¼Œæ²¡æƒ³åˆ°è¿™ä¹ˆå¥½ç”¨ï¼Œè¶…å‡ºé¢„æœŸ",
            "ä»·æ ¼å°è´µï¼Œä½†æè´¨å’Œåšå·¥æ˜æ˜¾æ¯”ä¾¿å®œè´§å¥½ï¼Œä¸€åˆ†é’±ä¸€åˆ†è´§",
            "åŠŸèƒ½ä¸ç®—å¤šï¼Œä½†æ¯ä¸€ä¸ªéƒ½å®ç”¨ï¼Œæ²¡æœ‰èŠ±é‡Œèƒ¡å“¨çš„ä¸œè¥¿",
            "ç‰©æµä¸€èˆ¬ï¼Œä½†é€è´§ä¸Šé—¨å¾ˆæ–¹ä¾¿ï¼Œçœäº†ä¸å°‘äº‹",
            "å®‰è£…æœ‰ç‚¹éº»çƒ¦ï¼Œä½†è¯´æ˜ä¹¦å¾ˆè¯¦ç»†ï¼Œæ…¢æ…¢å¼„ä¹Ÿèƒ½æå®š",
            "é¢œè‰²æ¯”æƒ³è±¡ä¸­æµ…ï¼Œä½†å¾ˆè€çœ‹ï¼Œè¶Šç”¨è¶Šå–œæ¬¢",
            "å£°éŸ³ä¸ç®—å¤§ï¼Œä½†æ¸…æ™°åº¦é«˜ï¼Œæ—¥å¸¸ç”¨è¶³å¤Ÿäº†",
            "å°ºç åå°ä¸€ç‚¹ï¼Œä½†ç‰ˆå‹å¾ˆå¥½ï¼Œæ¢å¤§ä¸€ç åˆšå¥½åˆé€‚",
            "åˆšç”¨æ—¶æœ‰è½»å¾®å¼‚å‘³ï¼Œé€šé£ä¸¤å¤©å°±æ²¡äº†ï¼Œä¸å½±å“ä½¿ç”¨",
            "æ“ä½œç•Œé¢æœ‰ç‚¹å¤æ‚ï¼Œä½†ç†Ÿæ‚‰åæ•ˆç‡å¾ˆé«˜ï¼Œç¦»ä¸å¼€äº†",
            "è¿™ä¸ªäº§å“éå¸¸å¥½ï¼Œæˆ‘å¾ˆæ»¡æ„", "è´¨é‡å¾ˆæ£’ï¼Œæ¨èè´­ä¹°", "ä½“éªŒè¶…å‡ºé¢„æœŸï¼Œå€¼å¾—æ‹¥æœ‰",
            "æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œä¸‹æ¬¡è¿˜ä¼šå†æ¥", "æ€§ä»·æ¯”é«˜ï¼Œéå¸¸åˆ’ç®—", "ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…å®Œå¥½",
            "æ•ˆæœæ˜¾è‘—ï¼Œç¡®å®æœ‰æ•ˆ", "å¤–è§‚è®¾è®¡å¾ˆæ¼‚äº®ï¼Œå¾ˆå–œæ¬¢", "ä½¿ç”¨ç®€å•æ–¹ä¾¿ï¼Œæ“ä½œæµç•…",
            "å‘³é“å¾ˆå¥½ï¼Œå®¶äººéƒ½å–œæ¬¢"
        ]

        # ä¼˜åŒ–åçš„è´Ÿé¢æ ·æœ¬ï¼ˆ15æ¡ç‹¬ç«‹æ ·æœ¬ï¼Œå«æ¨¡ç³Šè¡¨è¾¾å’Œå¤æ‚è¯­å¢ƒï¼‰
        negative_samples = [
            "æ‰‹æœºå‘çƒ­ä¸¥é‡ï¼Œç©10åˆ†é’Ÿæ¸¸æˆå°±çƒ«æ‰‹ï¼Œä¸æ•¢é•¿æ—¶é—´ç”¨",
            "å®¢æœåªä¼šè¯´å¥—è¯ï¼Œé—®é¢˜æ ¹æœ¬æ²¡è§£å†³ï¼Œä½“éªŒå¾ˆå·®",
            "å‘³é“å¤ªå’¸äº†ï¼Œæ–™åŒ…å…¨æ”¾æ ¹æœ¬æ²¡æ³•åƒï¼Œè¸©é›·äº†",
            "å¤–è§‚çœ‹ç€å»‰ä»·ï¼Œå¡‘æ–™æ„Ÿå¼ºï¼Œå’Œå›¾ç‰‡å·®è·å¤§",
            "ç”¨äº†ä¸åˆ°ä¸€å‘¨å°±å¡é¡¿ï¼Œåå°æ¸…äº†ä¹Ÿæ²¡ç”¨ï¼Œä¸æ¨è",
            "ä»·æ ¼è™šé«˜ï¼ŒåŒé…ç½®çš„å…¶ä»–ç‰Œå­ä¾¿å®œä¸€åŠï¼Œä¸å€¼è¿™ä¸ªä»·",
            "åŠŸèƒ½é¸¡è‚‹ï¼Œå¾ˆå¤šç”¨ä¸ä¸Šçš„è®¾è®¡ï¼Œå¾’å¢å¤æ‚åº¦",
            "ç‰©æµè¶…æ…¢ï¼Œæ˜¾ç¤ºä¸‰å¤©åˆ°ï¼Œç»“æœç­‰äº†ä¸€å‘¨æ‰æ”¶åˆ°",
            "å®‰è£…è¯´æ˜ä¸€å›¢ç³Ÿï¼Œçœ‹åŠå¤©çœ‹ä¸æ‡‚ï¼Œæœ€åæ‰¾äººå¸®å¿™æ‰è£…ä¸Š",
            "é¢œè‰²å‘é”™äº†ï¼Œé€€æ¢è¿˜è¦è‡ªå·±æ‰¿æ‹…è¿è´¹ï¼Œå¾ˆä¸åˆç†",
            "å£°éŸ³å¿½å¤§å¿½å°ï¼Œè°ƒèŠ‚ä¹Ÿä¸çµæ•ï¼Œå½±å“ä½¿ç”¨ä½“éªŒ",
            "å°ºç ä¸¥é‡ä¸å‡†ï¼Œæ ‡æ³¨XLå®é™…åƒMç ï¼Œé€€æ¢å¤ªéº»çƒ¦",
            "å¼‚å‘³ç‰¹åˆ«é‡ï¼Œæ”¾äº†ä¸€å‘¨è¿˜æœ‰å‘³ï¼Œä¸æ•¢ç»™å­©å­ç”¨",
            "æ“ä½œåäººç±»ï¼Œå¾ˆå¤šåŸºç¡€åŠŸèƒ½è—å¾—å¾ˆæ·±ï¼Œè€äººæ ¹æœ¬ä¸ä¼šç”¨",
            "å®£ä¼ è¯´é˜²æ°´ï¼Œç»“æœæº…äº†ç‚¹æ°´å°±åäº†ï¼Œè´¨é‡å ªå¿§",
            "è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·", "æœåŠ¡æ€åº¦æ¶åŠ£ï¼Œéå¸¸å¤±æœ›", "ä¸€ç‚¹ç”¨éƒ½æ²¡æœ‰ï¼Œæµªè´¹é’±",
            "ç‰©æµå¤ªæ…¢ï¼ŒåŒ…è£…ç ´æŸ", "ä½“éªŒå¾ˆå·®ï¼Œä¸ä¼šå†ä¹°äº†", "å‘³é“å¾ˆéš¾é—»ï¼Œæ— æ³•æ¥å—",
            "å¤–è§‚ç²—ç³™ï¼Œæœ‰ç‘•ç–µ", "æ“ä½œå¤æ‚ï¼Œä¸€ç‚¹éƒ½ä¸æ–¹ä¾¿", "æ•ˆæœå¾ˆå·®ï¼Œä¸å¦‚å®£ä¼ çš„å¥½",
            "æ€§ä»·æ¯”ä½ï¼Œä¸æ¨èè´­ä¹°"
        ]

        texts0 = positive_samples  + negative_samples 
        labels = [1] * len(positive_samples)  + [0] * len(negative_samples)   # 1:æ­£é¢, 0:è´Ÿé¢
        label_names = ["è´Ÿé¢", "æ­£é¢"]
        # ä¸­æ–‡é¢„å¤„ç†
        texts = [preprocess_text(text, is_chinese=True) for text in texts0]
        return texts, labels, label_names, "ä¸­æ–‡", texts0
    
    else:  # è‡ªå®šä¹‰æ–‡æœ¬
        return [], [], [], "ä¸­æ–‡", []

# ç‰¹å¾æå–æ¼”ç¤º
def demo_feature_extraction(texts, lang):
    """æ¼”ç¤ºè¯è¢‹æ¨¡å‹å’ŒTF-IDF"""
    st.subheader("ğŸ“ æ–‡æœ¬å‘é‡åŒ–ï¼šä»æ–‡å­—åˆ°æ•°å­—")
    
    # é€‰æ‹©å‘é‡åŒ–æ–¹æ³•
    vec_method = st.radio("é€‰æ‹©å‘é‡åŒ–æ–¹æ³•",
                          ["è¯è¢‹æ¨¡å‹ (CountVectorizer)", "TF-IDF (TfidfVectorizer)"],
                          horizontal=True)
    col1, col2 = st.columns(2)
    with col1:
    # è®¾ç½®å‚æ•°
        max_features = st.slider("æœ€å¤§ç‰¹å¾æ•°ï¼ˆé€‰å‡ºç°é¢‘ç‡æœ€é«˜çš„ n ä¸ªè¯ä½œä¸ºç‰¹å¾ï¼‰", 10, 500, 100)

    with col2:
        ngram_range = st.slider("N-gramèŒƒå›´ï¼ˆä½¿ç”¨è¿ç»­çš„ n ä¸ªè¯ç»„åˆï¼Œå¦‚'æœºå™¨å­¦ä¹ 'ä½œä¸ºä¸€ä¸ªç‰¹å¾ï¼‰", 1, 3, 1)

    st.info("""
- **è¯è¢‹æ¨¡å‹**ğŸ“¦ ï¼šæŠŠæ–‡æœ¬è½¬æ¢æˆæ•°å­—ç‰¹å¾çš„æ–¹æ³•ã€‚**æ ¸å¿ƒ**ä½œç”¨æ˜¯ç»Ÿè®¡è¯çš„å‡ºç°æ¬¡æ•°ï¼Œä¸è€ƒè™‘è¯çš„**é¡ºåºï¼Œè¯­æ³•**ï¼Œå³åªçœ‹æœ‰ä»€ä¹ˆè¯ã€å‡ºç°å¤šå°‘æ¬¡ã€‚
- **TF-IDF**ğŸ” ï¼šæ˜¯ä¸€ç§åŠ æƒç»Ÿè®¡æ–¹æ³•ã€‚**æ ¸å¿ƒ**ä½œç”¨æ˜¯ï¼šè¡¡é‡ä¸€ä¸ªè¯åœ¨æŸç¯‡æ–‡æ¡£ä¸­çš„**é‡è¦æ€§**ï¼Œæ—¢è€ƒè™‘è¯åœ¨å½“å‰æ–‡æ¡£çš„å‡ºç°é¢‘ç‡ï¼Œä¹Ÿå…¼é¡¾è¯åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„ç¨€ç¼ºæ€§ã€‚
        """)
    # åˆå§‹åŒ–å‘é‡å™¨
    if vec_method.startswith("è¯è¢‹"):
        vectorizer = CountVectorizer(
            max_features=max_features,
            ngram_range=(ngram_range, ngram_range),
            stop_words="english" if lang == "è‹±æ–‡" else None
        )
    else:
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=(ngram_range, ngram_range),
            stop_words="english" if lang == "è‹±æ–‡" else None
        )
    
    # æ‹Ÿåˆå¹¶è½¬æ¢
    X = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    
    # å±•ç¤ºç»“æœ
    st.write(f"å‘é‡åŒ–åå½¢çŠ¶: {X.shape} (æ ·æœ¬æ•° Ã— ç‰¹å¾æ•°)")
    
    # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬çš„ç‰¹å¾
    if len(texts) >= 5:
        st.subheader("æ ·æœ¬ç‰¹å¾ç¤ºä¾‹")
        df = pd.DataFrame(
            X[:5].toarray(), 
            columns=feature_names,
            index=[f"æ ·æœ¬ {i+1}" for i in range(5)]
        )
        st.dataframe(df.style.highlight_max(axis=1))
    
    # è§£é‡Šå‘é‡åŒ–åŸç†
    if vec_method.startswith("è¯è¢‹"):
        st.info(f""" 
    - å‘é‡åŒ–åå½¢çŠ¶ï¼š**{X.shape}** ï¼Œ **{X.shape[0]}** æ˜¯æ ·æœ¬æ•°ï¼Œ**{X.shape[1]}** æ˜¯ æœ€å¤§ç‰¹å¾æ•°ï¼ˆè¯æ±‡è¡¨çš„å¤§å°ï¼‰ 
    - æ¯ä¸€è¡Œå¯¹åº” 1 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—å¯¹åº” 1 ä¸ªç‰¹å¾ï¼Œå•å…ƒæ ¼é‡Œçš„æ•°å­—æ˜¯è¿™ä¸ªç‰¹å¾åœ¨è¯¥æ ·æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°
    - âœ… é»„è‰²å•å…ƒæ ¼é‡Œçš„ 1 ï¼šè¡¨ç¤ºè¿™ä¸ªç‰¹å¾åœ¨è¯¥æ ·æœ¬ä¸­å‡ºç°äº† 1 æ¬¡
    - âœ… å•å…ƒæ ¼é‡Œçš„ 0 ï¼šè¡¨ç¤ºè¿™ä¸ªç‰¹å¾åœ¨è¯¥æ ·æœ¬ä¸­æ²¡æœ‰å‡ºç°
        """)
    else:
        st.info(f""" 
    - å‘é‡åŒ–åå½¢çŠ¶ï¼š**{X.shape}** ï¼Œ **{X.shape[0]}** æ˜¯æ ·æœ¬æ•°ï¼Œ**{X.shape[1]}** æ˜¯ æœ€å¤§ç‰¹å¾æ•°ï¼ˆè¯æ±‡è¡¨çš„å¤§å°ï¼‰ 
    - æ¯ä¸€è¡Œå¯¹åº” 1 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—å¯¹åº” 1 ä¸ªç‰¹å¾ï¼Œå•å…ƒæ ¼é‡Œçš„æ•°å­—æ˜¯è¿™ä¸ªç‰¹å¾åœ¨è¯¥æ ·æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°
    - âœ… **TFï¼ˆè¯é¢‘ï¼‰**ï¼šæŸè¯åœ¨å•ç¯‡æ–‡æ¡£ä¸­å‡ºç°çš„**æ¬¡æ•° / è¯¥æ–‡æ¡£çš„æ€»è¯æ•°**ï¼Œåæ˜ è¯å¯¹å½“å‰æ–‡æ¡£çš„â€œè´¡çŒ®åº¦â€   
    - âœ… **IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰**ï¼š**log (æ€»æ–‡æ¡£æ•° / åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°)** ï¼Œåæ˜ è¯çš„ â€œç¨€ç¼ºæ€§â€ï¼ˆè¶Šç¨€æœ‰ï¼ŒIDF è¶Šé«˜ï¼Œé‡è¦æ€§è¶Šå¼ºï¼‰
    - âœ… **TF-IDF = TF Ã— IDF**ï¼šæœ€ç»ˆå¾—åˆ†è¶Šé«˜ï¼Œè¯´æ˜è¿™ä¸ªè¯æ˜¯å½“å‰æ–‡æ¡£çš„ â€œæ ¸å¿ƒç‰¹å¾è¯â€
    """)
    
    return X, vectorizer, lang

# æ–‡æœ¬é¢„æµ‹åŠŸèƒ½
def text_prediction_demo(model, vectorizer, label_names, lang):
    """æ¼”ç¤ºæ–‡æœ¬é¢„æµ‹"""
    # è¾“å…¥æ–‡æœ¬
    user_text = st.text_input("è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹:", "è¿™ä¸ªäº§å“å¾ˆå¥½ï¼Œæˆ‘éå¸¸æ»¡æ„" )
    
    if st.button("æ–‡æœ¬åˆ†ç±»é¢„æµ‹"):
        # é¢„å¤„ç†
        processed_text = preprocess_text(user_text, is_chinese=(lang == "ä¸­æ–‡"))
        # å‘é‡åŒ–
        text_vec = vectorizer.transform([processed_text])
        # é¢„æµ‹
        pred = model.predict(text_vec)[0]
        pred_proba = model.predict_proba(text_vec)[0].max()
        
        st.success(f"é¢„æµ‹ç»“æœ:  {st.session_state.en_label_names[pred]} / {st.session_state.cn_label_names[pred]}  (ç½®ä¿¡åº¦: {pred_proba:.2f})")

        st.subheader("å…³é”®ç‰¹å¾åˆ†æ")
        st.info("""åªæœ‰é€‰æ‹©**é€»è¾‘å›å½’**æ‰èƒ½æ˜¾ç¤ºå…³é”®ç‰¹å¾åˆ†æå›¾ï¼Œå› ä¸ºåªæœ‰ç³»æ•°çš„æ¨¡å‹æ‰èƒ½åˆ†æ â€œç‰¹å¾é‡è¦æ€§â€
- âœ… **é€»è¾‘å›å½’**ï¼šæœ‰coef_å±æ€§ â†’ coef_é‡Œå­˜çš„æ˜¯ â€œæ¯ä¸ªè¯ï¼ˆç‰¹å¾ï¼‰å¯¹ 4 ä¸ªç±»åˆ«ï¼ˆè®¡ç®—æœºå›¾å½¢å­¦ / æ›²æ£çƒç­‰ï¼‰çš„æƒé‡å€¼â€ï¼Œæ¯”å¦‚ â€œå›¾å½¢â€ è¿™ä¸ªè¯å¯¹ â€œè®¡ç®—æœºå›¾å½¢å­¦â€ ç±»åˆ«çš„ç³»æ•°ä¸ºæ­£ä¸”æ•°å€¼å¤§ï¼Œè¯´æ˜è¿™ä¸ªè¯èƒ½æ˜¾è‘—é¢„æµ‹è¯¥ç±»åˆ«ã€‚
- âŒ **æœ´ç´ è´å¶æ–¯**ï¼šæ²¡æœ‰coef_å±æ€§ â†’ æœ´ç´ è´å¶æ–¯æ˜¯åŸºäºæ¦‚ç‡çš„æ¨¡å‹ï¼Œä¸è®¡ç®—ç‰¹å¾ç³»æ•°ï¼Œå› æ­¤æ— æ³•é€šè¿‡coef_åˆ†æç‰¹å¾é‡è¦æ€§ã€‚
            """)        
        # æ˜¾ç¤ºé‡è¦ç‰¹å¾
        if hasattr(model, 'coef_'):
            # è·å–ç‰¹å¾é‡è¦æ€§
            coefs = model.coef_[0]
            feature_names = vectorizer.get_feature_names_out()
            
            # æ’åºå¹¶æ˜¾ç¤º
            top_n = min(10, len(feature_names))
            indices = np.argsort(np.abs(coefs))[-top_n:]
            top_features = [feature_names[i] for i in indices]
            top_coefs = [coefs[i] for i in indices]
            cols=st.columns([1,5,1])
            with cols[1]:            
                # å¯è§†åŒ–
                fig, ax = plt.subplots(figsize=(8, 6))
                sns.barplot(x=top_coefs, y=top_features, ax=ax)
                ax.set_title("å¯¹é¢„æµ‹å½±å“æœ€å¤§çš„ç‰¹å¾")
                st.pyplot(fig)
            st.info("""
- ğŸ‘‰ **ç‰¹å¾**å°±æ˜¯è¯è¢‹ / TF-IDF ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰è¯ï¼ˆæ¯”å¦‚ â€œgraphicsâ€ã€â€œhockeyâ€ã€â€œspaceâ€ ç­‰ï¼‰ï¼Œæ•°é‡ç­‰äºä½ è®¾ç½®çš„ â€œæœ€å¤§ç‰¹å¾æ•°â€ã€‚
- ğŸ‘‰ æœ€ç»ˆå¯è§†åŒ–çš„æ˜¯ â€œæƒé‡ç»å¯¹å€¼ Top10 çš„ç‰¹å¾â€ï¼Œè€Œéæ‰€æœ‰ç‰¹å¾ã€‚""")



# å„æ¨¡å—å®ç°
def text_introduction_section():
    """æ–‡æœ¬åˆ†æåŸºç¡€ä»‹ç»"""
    st.subheader("ğŸ“š æ–‡æœ¬åˆ†æåŸºç¡€")    
    st.markdown("### **ä»€ä¹ˆæ˜¯æ–‡æœ¬åˆ†æ:ğŸ“Š**")
   
    st.info("""
##### æ–‡æœ¬åˆ†ææ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­æå–æœ‰ä»·å€¼ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
1. **æ–‡æœ¬åˆ†ç±»**ï¼šå°†æ–‡æœ¬åˆ’åˆ†åˆ°é¢„å®šä¹‰ç±»åˆ«ã€‚ä¾‹å¦‚æ–°é—»å¹³å°è‡ªåŠ¨å°†ç¨¿ä»¶å½’ç±»ä¸ºâ€œæ—¶æ”¿â€ã€â€œå¨±ä¹â€ã€â€œä½“è‚²â€ç­‰æ ç›®
2. **æƒ…æ„Ÿåˆ†æ**ï¼šåˆ¤æ–­æ–‡æœ¬æƒ…æ„Ÿå€¾å‘ï¼Œå¦‚åˆ†æå°çº¢ä¹¦ç¾å¦†è¯„è®ºï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ã€ç›‘æµ‹æ–°è½¦è¯„æµ‹çš„è´¨ç–‘æƒ…ç»ª
3. **ä¸»é¢˜æå–**ï¼šè¯†åˆ«æ ¸å¿ƒä¸»é¢˜ï¼Œå¦‚èŒåœºè®ºå›çš„â€œåŠ ç­/è–ªèµ„/æ™‹å‡â€ã€æ”¿ç­–åé¦ˆçš„â€œå®æ–½ç»†åˆ™/å—ç›ŠèŒƒå›´â€
4. **å‘½åå®ä½“è¯†åˆ«**ï¼šè¯†åˆ«äººåã€åœ°åç­‰ï¼Œå¦‚åœ¨ç—…å†ä¸­æå–â€œå¼ ä¸‰/å† å¿ƒç—…â€ã€æ–°é—»ä¸­æå–â€œå·¥å•†é“¶è¡Œ/ä¸Šæµ·â€
    """)

    st.markdown("### **æ–‡æœ¬æ•°æ®çš„ç‰¹ç‚¹:ğŸ“Š**")   
    st.info("""
##### æ–‡æœ¬åˆ†ææ˜¯ä»éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­æå–æœ‰ä»·å€¼ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
1. **éç»“æ„åŒ–**ï¼šæ— å›ºå®šæ ¼å¼ï¼Œå¦‚å¾®ä¿¡èŠå¤©ã€å•†å“è¯„è®ºã€æ‰‹å†™ç—…å†ç­‰ï¼Œæ— ç»Ÿä¸€ç»“æ„ä¸è§„èŒƒå­—æ®µ
2. **é«˜ç»´åº¦**ï¼šè¯æ±‡è¡¨åºå¤§ï¼Œå¦‚å•†å“è¯„è®ºå«æ•°ä¸‡è¯æ±‡ï¼Œæ¯ä¸ªè¯æ±‡å‡å¯è§†ä¸ºä¸€ä¸ªæ•°æ®ç»´åº¦
3. **ç¨€ç–æ€§**ï¼šå¤šæ•°è¯æ±‡åœ¨å¤šæ•°æ–‡æœ¬ä¸­ä¸å‡ºç°ï¼Œå¦‚æ‰‹æœºè¯„è®ºå°‘ç¾å¦†è¯æ±‡ï¼Œç¾å¦†è¯„è®ºå°‘æ‰‹æœºè¯æ±‡
4. **è¯­ä¹‰å¤æ‚æ€§**ï¼šå­˜åœ¨ä¸€è¯å¤šä¹‰ã€æ­§ä¹‰ï¼Œå¦‚â€œè‹¹æœâ€å¯æŒ‡æ°´æœæˆ–å“ç‰Œï¼Œâ€œæœ‰ç‚¹æ„æ€â€å¯è¤’å¯è´¬ï¼Œéœ€ç»“åˆè¯­å¢ƒåˆ¤æ–­ã€‚
    """)
    # è®°å½•æ•°æ®ç”Ÿæˆæ“ä½œ
    st.session_state.text_analysis_records["text_introduction_section"].append({
        "timestamp": datetime.now().timestamp()
    })

def text_preprocessing_section():
    """æ–‡æœ¬é¢„å¤„ç†æ¨¡å—"""
    st.subheader("âœ‚ï¸ æ–‡æœ¬é¢„å¤„ç†")    
    st.markdown("""
    **é¢„å¤„ç†çš„ç›®çš„:**
    æ¸…æ´—æ–‡æœ¬æ•°æ®ï¼Œå»é™¤å™ªå£°ï¼Œæ ‡å‡†åŒ–æ ¼å¼ï¼Œä¸ºåç»­å‘é‡åŒ–åšå‡†å¤‡
    """)

    st.info("""
    **åŸºæœ¬æ­¥éª¤:**
    1. å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ— å…³ç¬¦å·
    2. å¤§å°å†™è½¬æ¢ï¼ˆè‹±æ–‡ï¼‰
    3. åˆ†è¯ï¼ˆå°†å¥å­æ‹†åˆ†ä¸ºè¯è¯­ï¼‰
    4. å»é™¤åœç”¨è¯ï¼ˆå¦‚"çš„"ã€"æ˜¯"ã€"the"ç­‰æ— å®é™…æ„ä¹‰çš„è¯ï¼‰
    5. è¯å½¢è¿˜åŸ/è¯å¹²æå–ï¼ˆè‹±æ–‡ï¼‰
    """)
    
    # æ¼”ç¤ºé¢„å¤„ç†æ•ˆæœ
    st.subheader("é¢„å¤„ç†æ•ˆæœæ¼”ç¤º")
    lang = st.radio("é€‰æ‹©è¯­è¨€", ["ä¸­æ–‡", "è‹±æ–‡"])
    
    if lang == "ä¸­æ–‡":
        raw_text = st.text_input("è¾“å…¥ä¸­æ–‡æ–‡æœ¬:", "å¤§å®¶å¥½ï¼ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œæˆ‘ä»¬å»å…¬å›­ç©å§ï¼")
        processed_text = preprocess_text(raw_text, is_chinese=True)
        st.write("**åŸå§‹æ–‡æœ¬**:", raw_text)
        st.write("**é¢„å¤„ç†å**:", processed_text)
        st.write("**åˆ†è¯ç»“æœ**:", " / ".join(jieba.cut(raw_text)))
    else:
        raw_text = st.text_input("è¾“å…¥è‹±æ–‡æ–‡æœ¬:", "Hello! Today is a beautiful day, let's go to the park!")
        processed_text = preprocess_text(raw_text)
        st.write("**åŸå§‹æ–‡æœ¬**:", raw_text)
        st.write("**é¢„å¤„ç†å**:", processed_text)
    
    st.info("""
    **ä¸­æ–‡vsè‹±æ–‡å¤„ç†å·®å¼‚:**
    - ä¸­æ–‡éœ€è¦ä¸“é—¨çš„åˆ†è¯å·¥å…·ï¼ˆå¦‚ **jieba** ï¼‰ï¼Œè‹±æ–‡å¯ç›´æ¥æŒ‰ç©ºæ ¼åˆ†å‰²
    - è‹±æ–‡æœ‰è¯å½¢å˜åŒ–ï¼ˆå¤æ•°ã€æ—¶æ€ç­‰ï¼‰ï¼Œéœ€è¦è¯å¹²æå–æˆ–è¯å½¢è¿˜åŸ
    - ä¸­è‹±æ–‡åœç”¨è¯è¡¨ä¸åŒ
    """)
    st.markdown("---")
    st.subheader("ğŸ”¥ ä¸­æ–‡åˆ†è¯ä¹‹æŒ‘æˆ˜ä¸å¯èƒ½ï¼")
    # å®šä¹‰éš¾åˆ†è¯çš„æµ‹è¯•å¥å­åˆ—è¡¨
    test_sentences = [
        "å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥",          
        "æ¬¢è¿æ–°è€å¸ˆç”Ÿå‰æ¥å°±é¤",    
        "æˆ‘æƒ³è¿‡è¿‡è¿‡å„¿è¿‡è¿‡çš„ç”Ÿæ´»",        
        "ä¸‹é›¨å¤©ç•™å®¢å¤©ç•™æˆ‘ä¸ç•™",   
        "ä¹’ä¹“çƒæ‹å–å®Œäº†",
        "åšæ ¸é…¸çš„é˜Ÿé•¿æ­»äº†",                      
    ]
    st.write("##### ğŸ”¤ é€‰æ‹©/è¾“å…¥æµ‹è¯•å¥å­")
    # é€‰æ‹©é¢„è®¾å¥å­
    selected_sentence = st.selectbox(
        "é€‰æ‹©éœ€è¦åˆ†è¯çš„å¥å­",
        test_sentences,
        index=0,
        help="é€‰æ‹©é¢„è®¾çš„æ˜“æ­§ä¹‰å¥å­æµ‹è¯•åˆ†è¯æ•ˆæœ"
    )
    # ç¡®å®šæœ€ç»ˆè¦åˆ†è¯çš„å¥å­
    target_sentence = selected_sentence

    # åˆ†è¯å¤„ç†ï¼ˆæ™®é€šåˆ†è¯ + ç²¾ç¡®æ¨¡å¼ + å…¨æ¨¡å¼ï¼‰
    st.write("##### ğŸ“Œ åˆ†è¯ç»“æœå¯¹æ¯”")
    st.caption("æ³¨ï¼šç»“å·´åˆ†è¯å·²å†…ç½®ä¸­æ–‡å¸¸ç”¨è¯åº“ï¼Œå¯¹ç”Ÿåƒ»è¯/äººåå¯è‡ªå®šä¹‰æ·»åŠ è¯åº“")
    col1, col2, col3 = st.columns([1.05,1.3,1.2])

    # 1. æ™®é€šåˆ†è¯ï¼ˆé»˜è®¤ç²¾ç¡®æ¨¡å¼ï¼‰
    default_cut = jieba.lcut(target_sentence)
    with col1:
        st.markdown("**é»˜è®¤ç²¾ç¡®æ¨¡å¼**--æœ€å¸¸ç”¨ï¼Œç²¾å‡†åˆ‡åˆ†")
        st.write(" / ".join(default_cut))

    # 2. å…¨æ¨¡å¼ï¼ˆæ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„åˆ†è¯ç»“æœï¼‰
    full_cut = jieba.lcut(target_sentence, cut_all=True)
    with col2:
        st.markdown("**å…¨æ¨¡å¼**--ç©·å°½æ‰€æœ‰å¯èƒ½ï¼Œæœ‰å†—ä½™")
        st.write(" / ".join(full_cut))

    # 3. æœç´¢å¼•æ“æ¨¡å¼ï¼ˆç²¾ç¡®æ¨¡å¼åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†ï¼‰
    search_cut = jieba.lcut_for_search(target_sentence)
    with col3:
        st.markdown("**æœç´¢å¼•æ“æ¨¡å¼**--é€‚åˆæœç´¢åœºæ™¯")
        st.write(" / ".join(search_cut))
        
    # è®°å½•æ•°æ®ç”Ÿæˆæ“ä½œ
    st.session_state.text_analysis_records["text_preprocessing_section"].append({
        "lang":lang,
        "timestamp": datetime.now().timestamp()
    })
    
def text_analysis_section():
    """æ–‡æœ¬åˆ†ç±»ä¸“é¡¹ï¼ˆé€‚é…5ä¸ªç±»åˆ«ï¼‰"""
    st.subheader("æ–‡æœ¬åˆ†ç±»æµç¨‹æ¼”ç¤º")
    st.write("### 1. ğŸ“Š æ–‡æœ¬åˆ†ç±»æ•°æ®å±•ç¤º")
    
    texts, labels, label_names, _ = load_sample_data("æ–°é—»ä¸»é¢˜åˆ†ç±»")
    # æ‰‹åŠ¨å®šä¹‰ä¸­æ–‡ç±»åˆ«åï¼ˆå’ŒJSONä¸­ä¸€è‡´ï¼‰
    cn_label_names = ["è®¡ç®—æœºå›¾å½¢å­¦", "æ‘©æ‰˜è½¦", "æ£’çƒè¿åŠ¨", "å¤ªç©ºç§‘å­¦", "æ”¿æ²»è®¨è®º"]
    
    st.write(f"ğŸ’¡ **æ•°æ®é›†ä¿¡æ¯: {len(texts)}ä¸ªæ ·æœ¬ï¼Œ{len(label_names)}ä¸ªç±»åˆ«**")
    
    if texts:
        category_data = {
            "ç±»åˆ«ç¼–å·": list(range(len(label_names))),  # åŠ¨æ€é€‚é…5ä¸ªç±»åˆ«ï¼ˆ0-4ï¼‰
            "ç±»åˆ«åç§°": label_names,  # æ›¿æ¢åŸst.session_state.en_label_namesï¼ˆé¿å…ä¾èµ–å¤–éƒ¨çŠ¶æ€ï¼‰
            "ä¸­æ–‡é‡Šä¹‰": cn_label_names  # æ›¿æ¢åŸst.session_state.cn_label_names
        }
        category_df = pd.DataFrame(category_data)
            
        st.dataframe(
            category_df,
            column_config={
                "ç±»åˆ«ç¼–å·": st.column_config.NumberColumn("ğŸ”¢ ç±»åˆ«ç¼–å·", width="small"),
                "ç±»åˆ«åç§°": st.column_config.TextColumn("ğŸ“ è‹±æ–‡åç§°", width="medium"),
                "ä¸­æ–‡é‡Šä¹‰": st.column_config.TextColumn("ğŸ‡¨ğŸ‡³ ä¸­æ–‡é‡Šä¹‰", width="medium")
            },
            hide_index=True,
            use_container_width=True
        )            
       
        # æ˜¾ç¤ºæ ·æœ¬
        st.write("**ğŸ“ æ ·æœ¬ç¤ºä¾‹**")
        sample_options = [
            f"æ ·æœ¬{idx} - {label_names[labels[idx]]}" 
            for idx in range(min(10, len(texts)))  # æœ€å¤šå±•ç¤ºå‰10æ¡æ ·æœ¬
        ]
        # åˆ›å»ºä¸‹æ‹‰åˆ—è¡¨ï¼Œé»˜è®¤é€‰ä¸­ç¬¬0æ¡
        selected_sample = st.selectbox(
            "é€‰æ‹©è¦æŸ¥çœ‹çš„æ ·æœ¬",
            options=sample_options,
            index=0,
            help="é€‰æ‹©ä¸åŒæ ·æœ¬æŸ¥çœ‹æ–‡æœ¬å’Œå¯¹åº”æ ‡ç­¾"
        )
        # è§£æé€‰ä¸­çš„æ ·æœ¬ç´¢å¼•
        sample_idx = sample_options.index(selected_sample)
        # å±•ç¤ºæ ·æœ¬å†…å®¹
        st.write(f"æ–‡æœ¬: {texts[sample_idx]}")
        # ========== ä¿®æ”¹ç‚¹3ï¼šæ ·æœ¬æ ‡ç­¾æ˜¾ç¤ºé€‚é…5ä¸ªç±»åˆ« ==========
        st.write(f"æ ‡ç­¾: {label_names[labels[sample_idx]]} | {cn_label_names[labels[sample_idx]]}")
        
        # ç‰¹å¾æå–æ¼”ç¤º
        st.write("### 2. ğŸ“Š æƒ…æ„Ÿåˆ†ææ–‡æœ¬ç‰¹å¾æå–") 
        X, vectorizer, lang = demo_feature_extraction(texts, "è‹±æ–‡")
        
        st.write("### 3. ğŸ“Š æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
        # ========== ä¿®æ”¹ç‚¹4ï¼šæ›´æ–°ä»»åŠ¡è¯´æ˜ä¸­çš„ç±»åˆ«æè¿°ï¼ˆ4ç±»â†’5ç±»ï¼‰ ==========
        st.info("""
        ##### ğŸ‘‰ ä»»åŠ¡è¯´æ˜
        åŸºäºåŒ…å« 500 ä¸ªæ ·æœ¬ã€è¦†ç›– â€œè®¡ç®—æœºå›¾å½¢å­¦â€ã€â€œæ‘©æ‰˜è½¦â€ã€â€œæ£’çƒè¿åŠ¨â€ã€â€œå¤ªç©ºç§‘å­¦â€ã€â€œæ”¿æ²»è®¨è®ºâ€ ç­‰ 5 ç±»ä¸»é¢˜çš„æ–°é—»æ•°æ®é›†ï¼Œç”¨**æœ´ç´ è´å¶æ–¯**æˆ–**é€»è¾‘å›å½’**æ¨¡å‹å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚
        """)    
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2)
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            labels,
            test_size=test_size,
            random_state=42, 
            stratify=labels  # åˆ†å±‚æŠ½æ ·ï¼Œä¿è¯5ä¸ªç±»åˆ«åœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸­åˆ†å¸ƒä¸€è‡´
        )

        # é€‰æ‹©æ¨¡å‹
        model_name = st.selectbox("é€‰æ‹©åˆ†ç±»æ¨¡å‹", ["æœ´ç´ è´å¶æ–¯ (MultinomialNB)", "é€»è¾‘å›å½’ (LogisticRegression)"])
        
        # åˆå§‹åŒ–æ¨¡å‹
        if model_name.startswith("æœ´ç´ "):
            model = MultinomialNB()
        else:
            model = LogisticRegression(max_iter=1000)
            
        model.fit(X_train, y_train)

        # é¢„æµ‹
        y_pred = model.predict(X_test)

        # è¯„ä¼°æŒ‡æ ‡
        st.write("### 4. ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
        acc = accuracy_score(y_test, y_pred)
        st.metric("å‡†ç¡®ç‡ (Accuracy)", f"{acc:.4f}")
        
        # åˆ†ç±»è¯¦ç»†æŠ¥å‘Š
        st.write("##### ğŸ“‹ æ–‡æœ¬åˆ†ç±»è¯¦ç»†æŠ¥å‘Š")
        # è§£æclassification_reportä¸ºDataFrame
        report_dict = classification_report(y_test, y_pred, target_names=label_names, output_dict=True)
        # å‰”é™¤æ— å…³è¡Œï¼ˆå¦‚accuracyï¼‰ï¼Œä¿ç•™ç±»åˆ«çº§æŒ‡æ ‡
        report_df = pd.DataFrame(report_dict).T.drop(["accuracy", "macro avg", "weighted avg"])
        # ä¿ç•™4ä½å°æ•°ï¼Œä¼˜åŒ–æ˜¾ç¤º
        report_df = report_df.round(4)
        
        # ========== ä¿®æ”¹ç‚¹5ï¼šåˆ†ç±»æŠ¥å‘Šä¸­æ’å…¥5ä¸ªä¸­æ–‡ç±»åˆ«å ==========
        report_df.insert(0, "ç±»åˆ«å(CN)", cn_label_names)
        # é‡ç½®ç´¢å¼•å¹¶å°†åŸç´¢å¼•ï¼ˆè‹±æ–‡ç±»åˆ«åï¼‰è½¬ä¸ºåˆ—
        report_df = report_df.reset_index().rename(columns={"index": "ç±»åˆ«å(EN)"})
        # é‡å‘½åæŒ‡æ ‡åˆ—ä¸ºä¸­æ–‡
        report_df.rename(columns={
            "precision": "ç²¾ç¡®ç‡",
            "recall": "å¬å›ç‡",
            "f1-score": "F1åˆ†æ•°",
            "support": "æ ·æœ¬æ•°"
        }, inplace=True)
        st.dataframe(report_df, use_container_width=True)
        
        # æ··æ·†çŸ©é˜µï¼ˆä¸­æ–‡æ ‡ç­¾ï¼Œé€‚é…5ä¸ªç±»åˆ«ï¼‰
        st.write("##### ğŸ” æ··æ·†çŸ©é˜µ")
        cols=st.columns([1,5,1])
        with cols[1]:
            # ========== ä¿®æ”¹ç‚¹6ï¼šæ··æ·†çŸ©é˜µé€‚é…5ä¸ªç±»åˆ«ï¼Œè°ƒæ•´å›¾è¡¨å¤§å°é¿å…æ‹¥æŒ¤ ==========
            fig, ax = plt.subplots(figsize=(10, 8))  # å¢å¤§å›¾è¡¨å°ºå¯¸ï¼ˆåŸ8,6â†’10,8ï¼‰
            cm = confusion_matrix(y_test, y_pred)
            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cn_label_names)
            disp.plot(ax=ax, cmap="Blues", text_kw={"size": 16})  # è°ƒå¤§å­—ä½“
            plt.title("æ··æ·†çŸ©é˜µï¼ˆä¸­æ–‡æ ‡ç­¾ï¼‰", fontsize=16)
            plt.xticks(rotation=15)  # æ ‡ç­¾æ—‹è½¬é¿å…é‡å 
            ax.set_xlabel('é¢„æµ‹å€¼', fontsize=14)
            ax.set_ylabel('çœŸå®å€¼', fontsize=14)
            st.pyplot(fig)
            
        st.write("### 5. ğŸ“Š æ–‡æœ¬åˆ†ç±»é¢„æµ‹")   
        # é¢„è®¾ä¾‹å¥ï¼ˆå¯¹åº”æ‘©æ‰˜è½¦ã€æ£’çƒã€å¤ªç©ºç±»ï¼‰
        example_texts = {
            "æ‘©æ‰˜è½¦ç±»ä¾‹å¥": "The motorcycle engine has a powerful 1000cc motor",
            "æ£’çƒç±»ä¾‹å¥": "The baseball player hit a home run in the game",
            "å¤ªç©ºç±»ä¾‹å¥": "The rocket launched into space to explore Mars"
        }
        # ä¸‹æ‹‰é€‰æ‹©ä¾‹å¥
        selected_example = st.selectbox(
            "é€‰æ‹©é¢„è®¾ä¾‹å¥",
            options=list(example_texts.keys()),
            index=0
        )
        if st.button("æ–‡æœ¬åˆ†ç±»é¢„æµ‹", type="primary"):
            # é¢„å¤„ç†
            processed_text = preprocess_text(selected_example, is_chinese=(lang == "ä¸­æ–‡"))
            # å‘é‡åŒ–
            text_vec = vectorizer.transform([processed_text])
            
            # é¢„æµ‹
            pred_idx = model.predict(text_vec)[0]  # é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
            pred_proba = model.predict_proba(text_vec)[0].max()  # æœ€é«˜ç½®ä¿¡åº¦
            
            # ========== æ ¸å¿ƒä¿®å¤2ï¼šä½¿ç”¨æ•°æ®é›†çš„çœŸå®ç±»åˆ«åï¼Œè€Œésession_state ==========
            pred_en = label_names[pred_idx]  # æ•°æ®é›†è¿”å›çš„è‹±æ–‡ç±»åˆ«
            pred_cn = cn_label_names[pred_idx]  # å¯¹åº”çš„ä¸­æ–‡åç§°
            
            # æ˜¾ç¤ºæ­£ç¡®çš„é¢„æµ‹ç»“æœ
            st.success(f"é¢„æµ‹ç»“æœ:  {pred_en} / {pred_cn}  (ç½®ä¿¡åº¦: {pred_proba:.2f})")
    
            # ========== æ ¸å¿ƒä¿®å¤2ï¼šæ­£ç¡®çš„ç‰¹å¾é‡è¦æ€§åˆ†æ ==========
            st.subheader("å…³é”®ç‰¹å¾åˆ†æ")
            if hasattr(model, 'coef_'):  # ä»…é€»è¾‘å›å½’æœ‰coef_å±æ€§
                st.info("âœ… é€»è¾‘å›å½’æ¨¡å‹ - æ˜¾ç¤ºå¯¹å½“å‰é¢„æµ‹ç±»åˆ«å½±å“æœ€å¤§çš„ç‰¹å¾")
                
                # ä¿®å¤ï¼šå–å½“å‰é¢„æµ‹ç±»åˆ«çš„ç³»æ•°ï¼Œè€Œéç¬¬ä¸€ä¸ªç±»åˆ«çš„ç³»æ•°
                coefs = model.coef_[pred_idx]
                feature_names = vectorizer.get_feature_names_out()
                
                # å–ç»å¯¹å€¼å‰10çš„ç‰¹å¾ï¼ˆå½±å“æœ€å¤§ï¼‰
                top_n = min(10, len(feature_names))
                # æŒ‰ç³»æ•°ç»å¯¹å€¼æ’åºï¼Œå–top_n
                indices = np.argsort(np.abs(coefs))[-top_n:]
                top_features = [feature_names[i] for i in indices]
                top_coefs = [coefs[i] for i in indices]
                
                # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
                cols = st.columns([1, 5, 1])
                with cols[1]:
                    fig, ax = plt.subplots(figsize=(8, 6))
                    sns.barplot(x=top_coefs, y=top_features, ax=ax, palette="coolwarm")
                    ax.set_title(f"å¯¹ã€Œ{pred_cn}ã€ç±»åˆ«å½±å“æœ€å¤§çš„ç‰¹å¾", fontsize=12)
                    ax.set_xlabel("ç‰¹å¾ç³»æ•°ï¼ˆæ­£è´Ÿè¡¨ç¤ºä¿ƒè¿›/æŠ‘åˆ¶ï¼‰", fontsize=10)
                    st.pyplot(fig)
                
                st.info("""
                ğŸ“Œ ç‰¹å¾ç³»æ•°è§£è¯»ï¼š
                - æ­£æ•°ï¼šè¯¥è¯è¶Šé¢‘ç¹ï¼Œè¶Šå€¾å‘äºé¢„æµ‹ä¸ºå½“å‰ç±»åˆ«ï¼›
                - è´Ÿæ•°ï¼šè¯¥è¯è¶Šé¢‘ç¹ï¼Œè¶Šä¸å€¾å‘äºé¢„æµ‹ä¸ºå½“å‰ç±»åˆ«ï¼›
                - ç»å¯¹å€¼è¶Šå¤§ï¼Œç‰¹å¾å¯¹åˆ†ç±»çš„å½±å“è¶Šå¼ºã€‚
                """)
            else:
                # æœ´ç´ è´å¶æ–¯æ— coef_å±æ€§ï¼Œå‹å¥½æç¤º
                st.warning("""
                âŒ æœ´ç´ è´å¶æ–¯æ¨¡å‹æ— æ³•æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§ï¼š
                - æœ´ç´ è´å¶æ–¯æ˜¯åŸºäºæ¦‚ç‡çš„æ¨¡å‹ï¼Œæ— ç‰¹å¾ç³»æ•°ï¼ˆcoef_ï¼‰å±æ€§ï¼›
                - å¦‚éœ€åˆ†æç‰¹å¾é‡è¦æ€§ï¼Œè¯·é€‰æ‹©ã€Œé€»è¾‘å›å½’ (LogisticRegression)ã€æ¨¡å‹ã€‚
                """)
            

    # è®°å½•æ•°æ®ç”Ÿæˆæ“ä½œ
    st.session_state.text_analysis_records["text_analysis_section"].append({
        "selected_sample":selected_sample,
        "test_size":test_size,
        "model_name":model_name,
        "timestamp": datetime.now().timestamp()
    })

def sentiment_analysis_section():
    """æƒ…æ„Ÿåˆ†æä¸“é¡¹æ¨¡å—"""
    st.subheader("ğŸ˜Š æƒ…æ„Ÿåˆ†æåŸºç¡€")
    
    st.markdown("""
    **ä»€ä¹ˆæ˜¯æƒ…æ„Ÿåˆ†æ?**
    æƒ…æ„Ÿåˆ†ææ˜¯æ–‡æœ¬åˆ†ç±»çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œä¸“æ³¨äºè¯†åˆ«æ–‡æœ¬ä¸­çš„ä¸»è§‚æƒ…æ„Ÿå€¾å‘ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
    - ææ€§åˆ†æï¼šæ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
    - æƒ…æ„Ÿå¼ºåº¦åˆ†æï¼šæƒ…æ„Ÿçš„å¼ºçƒˆç¨‹åº¦
    - æƒ…æ„Ÿç±»å‹åˆ†æï¼šå–œæ‚¦ã€æ„¤æ€’ã€æ‚²ä¼¤ç­‰å…·ä½“æƒ…æ„Ÿ
    
    **åº”ç”¨åœºæ™¯:**
    - äº§å“è¯„ä»·åˆ†æ
    - ç¤¾äº¤åª’ä½“æƒ…æ„Ÿç›‘æµ‹
    - èˆ†æƒ…åˆ†æ
    - å®¢æˆ·åé¦ˆå¤„ç†
    """)
    
    # ç®€å•æƒ…æ„Ÿåˆ†ææ¼”ç¤º
    st.subheader("æƒ…æ„Ÿåˆ†ææµç¨‹æ¼”ç¤º")
    texts, labels, label_names, _ , texts0= load_sample_data("ä¸­æ–‡æƒ…æ„Ÿåˆ†æ")

    # åˆ›å»ºæ•°æ®åˆ—è¡¨
    data = []
    for text in texts0[:5]:
        data.append({"æ–‡æœ¬å†…å®¹": text, "ç±»åˆ«": "æ­£é¢"})        
    for text in texts0[25:30]:
        data.append({"æ–‡æœ¬å†…å®¹": text, "ç±»åˆ«": "è´Ÿé¢"})
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(data)
    st.write("### 1. ğŸ“Š æƒ…æ„Ÿåˆ†æåŸå§‹æ ·æœ¬æ•°æ®å±•ç¤º")
    st.dataframe(df, use_container_width=True)

    data = []
    for text in texts[:5]:
        data.append({"æ–‡æœ¬å†…å®¹": text, "ç±»åˆ«": "æ­£é¢"})        
    for text in texts[25:30]:
        data.append({"æ–‡æœ¬å†…å®¹": text, "ç±»åˆ«": "è´Ÿé¢"})
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(data)
    st.write("### 2. ğŸ“Š æƒ…æ„Ÿåˆ†æé¢„å¤„ç†åæ ·æœ¬æ•°æ®å±•ç¤º")
    st.dataframe(df, use_container_width=True)

    st.write("### 3. ğŸ“Š æƒ…æ„Ÿåˆ†ææ–‡æœ¬ç‰¹å¾æå–")    
    X, vectorizer, lang = demo_feature_extraction(texts, "ä¸­æ–‡")    
    st.write("### 4. ğŸ“Š æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
    
    st.info("""
    ##### ğŸ‘‰ ä»»åŠ¡è¯´æ˜
    åŸºäºåŒ…å« 50 ä¸ªæ ·æœ¬ã€è¦†ç›–**æ­£é¢** / **è´Ÿé¢**ä¸¤ç±»æƒ…æ„Ÿå€¾å‘çš„è¯„è®ºæ–‡æœ¬æ•°æ®é›†ï¼Œç”¨æœ´ç´ è´å¶æ–¯æˆ–é€»è¾‘å›å½’æ¨¡å‹å®Œæˆæ–‡æœ¬æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚    """)    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2)
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        labels,
        test_size=test_size,
        random_state=42, stratify=labels
    )

    # é€‰æ‹©æ¨¡å‹
    model_name = st.selectbox("é€‰æ‹©åˆ†ç±»æ¨¡å‹", ["æœ´ç´ è´å¶æ–¯ (MultinomialNB)", "é€»è¾‘å›å½’ (LogisticRegression)"])
    
    # åˆå§‹åŒ–æ¨¡å‹
    if model_name.startswith("æœ´ç´ "):
        model = MultinomialNB()
    else:
        model = LogisticRegression(max_iter=1000)
        
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_test)

    # è¯„ä¼°æŒ‡æ ‡
    st.write("### 5. ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
    acc = accuracy_score(y_test, y_pred)
    st.metric("å‡†ç¡®ç‡ (Accuracy)", f"{acc:.4f}")
    
    # åˆ†ç±»è¯¦ç»†æŠ¥å‘Š
    st.write("##### ğŸ“‹ æƒ…æ„Ÿåˆ†ç±»è¯¦ç»†æŠ¥å‘Š")
    report_dict = classification_report(
        y_test, y_pred, 
        target_names=label_names, 
        output_dict=True
    )

    # å‰”é™¤æ— å…³è¡Œï¼Œä¿ç•™æ­£è´Ÿä¸¤ç±»
    report_df = pd.DataFrame(report_dict).T.drop(["accuracy", "macro avg", "weighted avg"])
    report_df = report_df.round(4)

    # é‡ç½®ç´¢å¼•ï¼Œè‹±æ–‡ç±»åˆ«ååˆ—
    report_df = report_df.reset_index().rename(columns={"index": "æƒ…æ„Ÿç±»åˆ«"})
    # é‡å‘½åæŒ‡æ ‡åˆ—ä¸ºä¸­æ–‡
    report_df.rename(columns={
        "precision": "ç²¾ç¡®ç‡",
        "recall": "å¬å›ç‡",
        "f1-score": "F1åˆ†æ•°",
        "support": "æ ·æœ¬æ•°"  
    }, inplace=True)
    st.dataframe(report_df, use_container_width=True)

    
    # æ··æ·†çŸ©é˜µï¼ˆä¸­æ–‡æ ‡ç­¾ï¼Œæ ¸å¿ƒä¿®æ”¹ï¼‰
    st.write("##### ğŸ” æ··æ·†çŸ©é˜µ")
    cols=st.columns([1,3,1])
    with cols[1]:
        fig, ax = plt.subplots(figsize=(8, 6))
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
        disp.plot(ax=ax, cmap="Blues",text_kw={"size": 30})
        ax.grid(True, linestyle='--', linewidth=0.5, color='gray', alpha=0.7)
        plt.title("æ··æ·†çŸ©é˜µ", fontsize=14)
        ax.set_xlabel('é¢„æµ‹å€¼',fontsize=12)
        ax.set_ylabel('çœŸå®å€¼',fontsize=12)
        st.pyplot(fig)
    
    # å®æ—¶é¢„æµ‹
    st.write("### 6. ğŸ“Š æ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹")
    user_comment = st.selectbox("é€‰æ‹©å•†å“è¯„è®º",
                                ["æ‰‹æœºå‘çƒ­ä¸¥é‡ï¼Œç©10åˆ†é’Ÿæ¸¸æˆå°±çƒ«æ‰‹ï¼Œä¸æ•¢é•¿æ—¶é—´ç”¨",
                                 "ç¬¬ä¸€æ¬¡ç”¨è¿™ä¸ªç‰Œå­ï¼Œæ²¡æƒ³åˆ°è¿™ä¹ˆå¥½ç”¨ï¼Œè¶…å‡ºé¢„æœŸ",
                                 "è™½ç„¶å‘è´§æ…¢äº†ä¸¤å¤©ï¼Œä½†åŒ…è£…å¾ˆç”¨å¿ƒï¼Œäº§å“æ²¡ç‘•ç–µï¼Œæ»¡æ„",
                                 "åŠŸèƒ½é¸¡è‚‹ï¼Œå¾ˆå¤šç”¨ä¸ä¸Šçš„è®¾è®¡ï¼Œå¾’å¢å¤æ‚åº¦"])
    if st.button("åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"):
        processed = preprocess_text(user_comment, is_chinese=True)
        vec = vectorizer.transform([processed])
        pred = model.predict(vec)[0]
        st.success(f"æƒ…æ„Ÿé¢„æµ‹: {label_names[pred]}")

    # è®°å½•æ•°æ®ç”Ÿæˆæ“ä½œ
    st.session_state.text_analysis_records["sentiment_analysis_section"].append({
        "test_size":test_size,
        "model_name":model_name,
        "user_comment":user_comment,
        "timestamp": datetime.now().timestamp()
    })

def native_bys_section():
    # é¡µé¢æ ‡é¢˜
    st.header('ğŸ›¡ï¸ æœ´ç´ è´å¶æ–¯ç®—æ³•åº”ç”¨-è¯ˆéª—çŸ­ä¿¡è¯†åˆ«')
    st.subheader('ğŸ“š æœ´ç´ è´å¶æ–¯ç®—æ³•åŠåŸºæœ¬æ¦‚å¿µ')
    st.info("""
    æœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†å’Œç‰¹å¾æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾çš„åˆ†ç±»ç®—æ³•ï¼Œåœ¨æ–‡æœ¬åˆ†ç±»ã€åƒåœ¾é‚®ä»¶è¿‡æ»¤ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚

    ##### ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ
    1. **è´å¶æ–¯å®šç†**ï¼šæè¿°äº†åéªŒæ¦‚ç‡ä¸å…ˆéªŒæ¦‚ç‡ã€ä¼¼ç„¶æ¦‚ç‡çš„å…³ç³»ï¼Œå…¬å¼ä¸ºï¼š
       $P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}$
       å…¶ä¸­ï¼š
       - $P(C|X)$ï¼šåéªŒæ¦‚ç‡ï¼ˆå·²çŸ¥ç‰¹å¾Xæ—¶ï¼Œç±»åˆ«Cçš„æ¦‚ç‡ï¼‰
       - $P(C)$ï¼šå…ˆéªŒæ¦‚ç‡ï¼ˆç±»åˆ«Cçš„å›ºæœ‰æ¦‚ç‡ï¼‰
       - $P(X|C)$ï¼šä¼¼ç„¶æ¦‚ç‡ï¼ˆç±»åˆ«Cä¸­å‡ºç°ç‰¹å¾Xçš„æ¦‚ç‡ï¼‰
       - $P(X)$ï¼šè¯æ®å› å­ï¼ˆç‰¹å¾Xçš„è¾¹é™…æ¦‚ç‡ï¼‰

    2. **ç‰¹å¾æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾**ï¼šå‡è®¾æ‰€æœ‰ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œç®€åŒ–è®¡ç®—å¤æ‚åº¦ï¼Œè¿™ä¹Ÿæ˜¯"æœ´ç´ "ï¼ˆNaiveï¼‰ä¸€è¯çš„ç”±æ¥ã€‚

    3. **æ–‡æœ¬åˆ†ç±»åº”ç”¨**ï¼š
       - å°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯è¯­ä½œä¸ºç‰¹å¾
       - è®¡ç®—ä¸åŒç±»åˆ«ï¼ˆå¦‚æ­£å¸¸/è¯ˆéª—çŸ­ä¿¡ï¼‰ä¸­è¯è¯­å‡ºç°çš„æ¦‚ç‡
       - é€šè¿‡åéªŒæ¦‚ç‡æ¯”è¾ƒåˆ¤æ–­æ–‡æœ¬ç±»åˆ«
    """)
    st.markdown("---")
    
    st.subheader('ğŸ“ åº”ç”¨åœºæ™¯è¯´æ˜')
    st.markdown("""
    éšç€ç§»åŠ¨äº’è”ç½‘çš„å‘å±•ï¼Œè¯ˆéª—çŸ­ä¿¡å·²æˆä¸ºå½±å“ç”¨æˆ·è´¢äº§å®‰å…¨çš„é‡è¦å¨èƒã€‚å¸¸è§è¯ˆéª—æ‰‹æ®µåŒ…æ‹¬ï¼š
    - **å†’å……å…¬æ£€æ³•**ï¼šä»¥è´¦æˆ·å¼‚å¸¸ã€æ¶‰å«Œè¿æ³•ç­‰ç†ç”±è¦æ±‚è½¬è´¦
    - **ä¸­å¥–è¯ˆéª—**ï¼šå£°ç§°ä¸­å¥–éœ€å…ˆç¼´çº³æ‰‹ç»­è´¹
    - **é‡‘èè¯ˆéª—**ï¼šä½æ¯è´·æ¬¾ã€ä¿¡ç”¨å¡æé¢ç­‰è¯±é¥µ
    - **å†’å……ç†Ÿäºº**ï¼šä¼ªè£…æˆé¢†å¯¼ã€äº²å‹è¦æ±‚è½¬è´¦
    - **é’“é±¼é“¾æ¥**ï¼šé€šè¿‡çŸ­ä¿¡é“¾æ¥çªƒå–ä¸ªäººä¿¡æ¯
    
    æœ¬ç³»ç»Ÿé‡‡ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•ï¼Œé€šè¿‡åˆ†æçŸ­ä¿¡å†…å®¹ç‰¹å¾ï¼Œè‡ªåŠ¨è¯†åˆ«è¯ˆéª—çŸ­ä¿¡ï¼Œä¿æŠ¤ç”¨æˆ·è´¢äº§å®‰å…¨ã€‚
    """)
    st.markdown("---")
    
    # å‡†å¤‡æ•°æ®
    train_data, normal_sms, fraud_sms = native_bys.prepare_training_data()
    
    # ä½¿ç”¨sklearnçš„train_test_splitåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ8:2æ¯”ä¾‹ï¼‰
    train_set, test_set = train_test_split(
        train_data,
        test_size=0.2,  # æµ‹è¯•é›†å æ¯”20%
        random_state=42,  # å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
        stratify=[d["label"] for d in train_data]  # æŒ‰æ ‡ç­¾åˆ†å±‚æŠ½æ ·ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
    )
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
    with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
        model_params = native_bys.train_model(train_set)
    
    # å±•ç¤ºæ•°æ®ç»Ÿè®¡ï¼ˆåŒ…å«æ•°æ®é›†åˆ’åˆ†ä¿¡æ¯ï¼‰
    native_bys.show_data_statistics(normal_sms, fraud_sms, len(train_set), len(test_set))
    
    # å±•ç¤ºå…³é”®è¯åˆ†æ
    native_bys.show_keyword_analysis(model_params)
    
    # å±•ç¤ºæ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
    native_bys.evaluate_model(model_params, test_set)
    
    # ä¸»äº¤äº’ç•Œé¢
    user_guess=native_bys.main_interface(model_params)
    
    # è®°å½•æ•°æ®ç”Ÿæˆæ“ä½œ
    st.session_state.text_analysis_records["native_bys_section"].append({
        "user_guess":user_guess,
        "timestamp": datetime.now().timestamp()
    })

def quiz_section():
    st.header("ğŸ¯ æ–‡æœ¬åˆ†ææ¦‚å¿µæµ‹éªŒ")
    st.write("è¯·å®Œæˆä»¥ä¸‹5é“å•é€‰é¢˜ï¼Œå…¨éƒ¨ç­”å®Œåå¯æäº¤æŸ¥çœ‹ç»“æœ")
    
    # å®šä¹‰æµ‹éªŒé¢˜ç›®ã€é€‰é¡¹ã€æ­£ç¡®ç­”æ¡ˆåŠè§£æï¼ˆèšç„¦æ–‡æœ¬åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æï¼‰
    quiz_data = [
        {
            "question": "1. æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œè¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ",
            "options": [
                "A. ä¿ç•™æ–‡æœ¬ä¸­è¯è¯­çš„é¡ºåºå’Œè¯­æ³•ç»“æ„",
                "B. å°†æ–‡æœ¬è¡¨ç¤ºä¸ºè¯æ±‡å‡ºç°é¢‘ç‡çš„å‘é‡ï¼Œå¿½ç•¥è¯åº",
                "C. åªèƒ½å¤„ç†è‹±æ–‡æ–‡æœ¬ï¼Œæ— æ³•å¤„ç†ä¸­æ–‡åˆ†è¯",
                "D. è‡ªåŠ¨æå–æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘"
            ],
            "correct": "B",
            "explanation": "è¯è¢‹æ¨¡å‹å°†æ–‡æœ¬è§†ä¸ºè¯æ±‡çš„é›†åˆï¼Œé€šè¿‡ç»Ÿè®¡æ¯ä¸ªè¯çš„å‡ºç°é¢‘ç‡æ„å»ºç‰¹å¾å‘é‡ï¼Œä¸è€ƒè™‘è¯è¯­çš„é¡ºåºå’Œè¯­æ³•å…³ç³»ï¼Œæ˜¯æ–‡æœ¬åˆ†ç±»ä¸­æœ€åŸºç¡€çš„ç‰¹å¾æå–æ–¹æ³•ã€‚"
        },
        {
            "question": "2. æƒ…æ„Ÿåˆ†æä¸æ™®é€šæ–‡æœ¬åˆ†ç±»çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Ÿ",
            "options": [
                "A. æƒ…æ„Ÿåˆ†æåªèƒ½å¤„ç†ä¸­æ–‡ï¼Œæ™®é€šæ–‡æœ¬åˆ†ç±»å¤„ç†è‹±æ–‡",
                "B. æƒ…æ„Ÿåˆ†æä¸“æ³¨äºè¯†åˆ«ä¸»è§‚æƒ…æ„Ÿå€¾å‘ï¼ˆå¦‚æ­£è´Ÿå‘ï¼‰ï¼Œæ™®é€šåˆ†ç±»ä¾§é‡å®¢è§‚ç±»åˆ«åˆ’åˆ†",
                "C. æƒ…æ„Ÿåˆ†æä¸éœ€è¦é¢„å¤„ç†ï¼Œæ™®é€šæ–‡æœ¬åˆ†ç±»éœ€è¦åˆ†è¯",
                "D. æƒ…æ„Ÿåˆ†æåªèƒ½ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•"
            ],
            "correct": "B",
            "explanation": "æƒ…æ„Ÿåˆ†ææ˜¯æ–‡æœ¬åˆ†ç±»çš„ç‰¹æ®Šå½¢å¼ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯è¯†åˆ«æ–‡æœ¬ä¸­çš„ä¸»è§‚æƒ…æ„Ÿï¼ˆå¦‚æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§ï¼‰ï¼Œè€Œæ™®é€šæ–‡æœ¬åˆ†ç±»æ›´å…³æ³¨å®¢è§‚ç±»åˆ«çš„åˆ’åˆ†ï¼ˆå¦‚æ–°é—»ä¸»é¢˜ã€é‚®ä»¶ç±»å‹ç­‰ï¼‰ã€‚"
        },
        {
            "question": "3. TF-IDFç‰¹å¾æå–ä¸­ï¼ŒIDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰çš„ä½œç”¨æ˜¯ï¼Ÿ",
            "options": [
                "A. æƒ©ç½šåœ¨å¤šæ•°æ–‡æ¡£ä¸­é¢‘ç¹å‡ºç°çš„å¸¸è§è¯ï¼ˆå¦‚â€œçš„â€â€œæ˜¯â€ï¼‰",
                "B. å¢åŠ é«˜é¢‘è¯çš„æƒé‡ï¼Œçªå‡ºå…¶é‡è¦æ€§",
                "C. ç¡®ä¿æ¯ä¸ªæ–‡æœ¬çš„ç‰¹å¾å‘é‡é•¿åº¦ç›¸åŒ",
                "D. è‡ªåŠ¨å»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šç¬¦å·å’Œæ•°å­—"
            ],
            "correct": "A",
            "explanation": "IDFé€šè¿‡è®¡ç®—â€œlog(æ€»æ–‡æ¡£æ•°/åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°)â€ï¼Œé™ä½åœ¨å¤šæ•°æ–‡æ¡£ä¸­éƒ½å‡ºç°çš„å¸¸è§è¯ï¼ˆå¦‚åœç”¨è¯ï¼‰çš„æƒé‡ï¼ŒåŒæ—¶æå‡åœ¨å°‘æ•°æ–‡æ¡£ä¸­å‡ºç°çš„ç¨€æœ‰è¯çš„æƒé‡ï¼Œæ›´èƒ½åæ˜ è¯çš„åŒºåˆ†åº¦ã€‚"
        },
        {
            "question": "4. ä»¥ä¸‹å“ªç§æƒ…å†µå¯èƒ½å¯¼è‡´æ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„æµ‹è¯•å‡†ç¡®ç‡è¿œä½äºè®­ç»ƒå‡†ç¡®ç‡ï¼Ÿ",
            "options": [
                "A. è®­ç»ƒæ•°æ®é‡è¿‡å¤§",
                "B. æ¨¡å‹å‡ºç°è¿‡æ‹Ÿåˆï¼Œè¿‡åº¦å­¦ä¹ è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°",
                "C. ä½¿ç”¨äº†TF-IDFè€Œéè¯è¢‹æ¨¡å‹",
                "D. æµ‹è¯•é›†ä¸è®­ç»ƒé›†åˆ†å¸ƒä¸€è‡´"
            ],
            "correct": "B",
            "explanation": "è¿‡æ‹Ÿåˆæ˜¯æ–‡æœ¬åˆ†ç±»ä¸­å¸¸è§çš„é—®é¢˜ï¼Œè¡¨ç°ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°æå¥½ï¼Œä½†åœ¨æœªè§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°å·®ï¼Œå› ä¸ºæ¨¡å‹è¿‡åº¦å­¦ä¹ äº†è®­ç»ƒæ•°æ®ä¸­çš„ç»†èŠ‚ï¼ˆåŒ…æ‹¬å™ªå£°ï¼‰ï¼Œè€Œæ²¡æœ‰æŠ“ä½é€šç”¨è§„å¾‹ã€‚"
        },
        {
            "question": "5. ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ä¸­ï¼Œåˆ†è¯çš„ä¸»è¦ç›®çš„æ˜¯ï¼Ÿ",
            "options": [
                "A. å°†è‹±æ–‡å•è¯è½¬æ¢ä¸ºä¸­æ–‡ç¿»è¯‘",
                "B. å»é™¤æ–‡æœ¬ä¸­çš„æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦",
                "C. å°†è¿ç»­çš„ä¸­æ–‡å¥å­æ‹†åˆ†ä¸ºæœ‰æ„ä¹‰çš„è¯è¯­å•å…ƒï¼Œä¾¿äºåç»­ç‰¹å¾æå–",
                "D. ç›´æ¥è®¡ç®—æ–‡æœ¬çš„æƒ…æ„Ÿå¾—åˆ†"
            ],
            "correct": "C",
            "explanation": "ä¸­æ–‡æ–‡æœ¬æ²¡æœ‰åƒè‹±æ–‡é‚£æ ·çš„ç©ºæ ¼åˆ†éš”ï¼Œåˆ†è¯æ˜¯å°†è¿ç»­çš„å­—ç¬¦åºåˆ—æ‹†åˆ†ä¸ºæœ‰æ„ä¹‰çš„è¯è¯­ï¼ˆå¦‚å°†â€œå—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥â€æ‹†åˆ†ä¸ºâ€œå—äº¬å¸‚ / é•¿æ±Ÿå¤§æ¡¥â€ï¼‰ï¼Œæ˜¯ä¸­æ–‡æ–‡æœ¬ç‰¹å¾æå–çš„å¿…è¦å‰ç½®æ­¥éª¤ã€‚"
        }
    ]
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å­˜å‚¨ç”¨æˆ·ç­”æ¡ˆ
    if "text_analysis_user_answers" not in st.session_state:
        st.session_state.text_analysis_user_answers = [None] * len(quiz_data)
    
    # æ˜¾ç¤ºæ‰€æœ‰é¢˜ç›®å’Œé€‰é¡¹ï¼ˆåˆå§‹æ— é€‰ä¸­çŠ¶æ€ï¼‰
    for i, item in enumerate(quiz_data):
        st.markdown(f"**{item['question']}**")
        # è®¾ç½®é»˜è®¤å€¼ä¸ºNoneå®ç°åˆå§‹æ— é€‰ä¸­çŠ¶æ€ï¼Œé€šè¿‡ä¼šè¯çŠ¶æ€ä¿å­˜ç­”æ¡ˆ
        answer = st.radio(
            "é€‰æ‹©ç­”æ¡ˆ:",
            item["options"],
            key=f"text_quiz_{i}",
            index=None,  # å…³é”®ï¼šåˆå§‹æ— é€‰ä¸­é¡¹
            label_visibility="collapsed"
        )
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„ç­”æ¡ˆï¼ˆæå–é€‰é¡¹å­—æ¯A/B/Cï¼‰
        if answer is not None:
            st.session_state.text_analysis_user_answers[i] = answer[0]
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é¢˜ç›®éƒ½å·²ä½œç­”
    all_answered = all(ans is not None for ans in st.session_state.text_analysis_user_answers)
    
    # æäº¤æŒ‰é’®ï¼šåªæœ‰å…¨éƒ¨ç­”å®Œæ‰å¯ç”¨
    submit_btn = st.button(
        "æäº¤ç­”æ¡ˆ", 
        key="submit_text_quiz",
        disabled=not all_answered  # æœªç­”å®Œæ—¶ç¦ç”¨
    )
    
    # æœªç­”å®Œæ—¶æ˜¾ç¤ºæç¤º
    if not all_answered:
        st.info("è¯·å®Œæˆæ‰€æœ‰5é“é¢˜ç›®åå†æäº¤")
    
    # å¤„ç†æäº¤
    if submit_btn and all_answered:
        # è®¡ç®—å¾—åˆ†å’Œé”™è¯¯é¢˜ç›®
        score = 0
        results = []
        incorrect_questions = []
        for i, item in enumerate(quiz_data):
            is_correct = st.session_state.text_analysis_user_answers[i] == item["correct"]
            if is_correct:
                score += 20  # æ¯é¢˜20åˆ†
            else:
                incorrect_questions.append({
                    "topic": item["question"], 
                    "user_answer": st.session_state.text_analysis_user_answers[i]
                })

            results.append({
                "question": item["question"],
                "user_answer": st.session_state.text_analysis_user_answers[i],
                "correct_answer": item["correct"],
                "is_correct": is_correct,
                "explanation": item["explanation"]
            })
        
        # ç¡®ä¿ç»“æœè®°å½•çš„ä¼šè¯çŠ¶æ€å­˜åœ¨
        if "text_analysis_records" not in st.session_state:
            st.session_state.text_analysis_records = {}
        
        # è®°å½•æµ‹éªŒç»“æœï¼ˆæ·»åŠ æ—¶é—´æˆ³ï¼‰
        st.session_state.text_analysis_records["text_analysis_quiz"] = {
            "score": score,
            "incorrect_questions": incorrect_questions,
            "timestamp": datetime.now().timestamp()
        }
       
        # æ˜¾ç¤ºå¾—åˆ†
        st.success(f"ğŸ“Š æµ‹éªŒå®Œæˆï¼ä½ çš„å¾—åˆ†æ˜¯ï¼š{score}åˆ†")
        st.write("### ç­”æ¡ˆè§£æï¼š")
        
        # æ˜¾ç¤ºæ¯é¢˜ç»“æœ
        for res in results:
            # ä½¿ç”¨emojiå’Œæ–‡å­—æ ‡è®°æ­£ç¡®/é”™è¯¯çŠ¶æ€
            if res["is_correct"]:
                status_text = "âœ… æ­£ç¡®"
            else:
                status_text = "âŒ é”™è¯¯"
            
            with st.expander(f"{res['question']} {status_text}"):
                if res["is_correct"]:
                    st.success(f"ä½ çš„ç­”æ¡ˆï¼š{res['user_answer']}ï¼ˆæ­£ç¡®ï¼‰")
                else:
                    st.error(f"ä½ çš„ç­”æ¡ˆï¼š{res['user_answer']}ï¼ˆé”™è¯¯ï¼‰")
                    st.info(f"æ­£ç¡®ç­”æ¡ˆï¼š{res['correct_answer']}")
                st.write(f"è§£æï¼š{res['explanation']}")

        # å‡†å¤‡AIåˆ†æçš„è¾“å…¥
        incorrect_topics = [
            res["question"] for res in results if not res["is_correct"]
        ]
        
        analysis_prompt = f"""
        ä»¥ä¸‹æ˜¯å­¦ç”Ÿåœ¨æ–‡æœ¬åˆ†ææµ‹éªŒä¸­çš„ç­”é¢˜æƒ…å†µï¼š
        - æ€»å¾—åˆ†ï¼š{score}åˆ†
        - é”™è¯¯é¢˜ç›®ï¼š{len(incorrect_topics)}é“
        - é”™è¯¯çŸ¥è¯†ç‚¹ï¼š{'; '.join(incorrect_topics) if incorrect_topics else 'æ— '}
        
        è¯·åˆ†æè¯¥å­¦ç”Ÿçš„çŸ¥è¯†æŒæ¡æƒ…å†µï¼ŒæŒ‡å‡ºæœªæŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶ç»™å‡ºå…·ä½“çš„å­¦ä¹ å»ºè®®å’ŒæŒ‡å¯¼æ–¹å‘ï¼Œå¸®åŠ©å­¦ç”Ÿé’ˆå¯¹æ€§æå‡ã€‚
        ç­”æ¡ˆå¿…é¡»æ§åˆ¶åœ¨450å­—ä»¥å†…
        """
        
        # è°ƒç”¨AIåˆ†æ
        with st.spinner("AIæ­£åœ¨åˆ†æä½ çš„ç­”é¢˜æƒ…å†µ..."):
            ai_analysis = ask_ai_assistant(analysis_prompt, "æ–‡æœ¬åˆ†ææµ‹éªŒåˆ†æ")
        
        # æ˜¾ç¤ºAIåˆ†æç»“æœ
        st.write("### ğŸ¤– AIå­¦ä¹ è¯Šæ–­ï¼š")
        st.info(ai_analysis)       
  
    return "æ¦‚å¿µæµ‹éªŒæ¨¡å—ï¼šå®Œæˆ5é¢˜å•é€‰é¢˜æµ‹è¯•"


# ä¸»ç¨‹åº
def main():
    init_session_state()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'section' not in st.session_state:
        st.session_state.section = "æ–‡æœ¬åˆ†æåŸºç¡€"

    if "label_mapping" not in st.session_state:
        st.session_state.label_mapping = {
            "comp.graphics": "è®¡ç®—æœºå›¾å½¢å­¦",
            "rec.sport.hockey": "ä¼‘é—²ä½“è‚²-æ›²æ£çƒ",
            "sci.space": "ç§‘å­¦-èˆªå¤©/å¤ªç©º",
            "talk.politics.misc": "è®¨è®º-æ”¿æ²»æ‚é¡¹"
        }
        # æ‹†åˆ†å‡ºè‹±æ–‡/ä¸­æ–‡ç±»åˆ«ååˆ—è¡¨ï¼ˆå­˜å…¥Sessionï¼Œæ–¹ä¾¿ç›´æ¥è°ƒç”¨ï¼‰
        st.session_state.en_label_names = list(st.session_state.label_mapping.keys())
        st.session_state.cn_label_names = list(st.session_state.label_mapping.values())

    # è®°å½•æ¨¡å—è®¿é—®é¡ºåº
    current_section = st.session_state.section
    st.session_state.text_analysis_records["module_sequence"].append(current_section)
    if current_section not in st.session_state.text_analysis_records["module_timestamps"]:
        st.session_state.text_analysis_records["module_timestamps"][current_section] = {
            "enter_time": time.time()
        } 
        
    # ä¾§è¾¹æ å¯¼èˆª
    st.sidebar.title("å¯¼èˆªèœå•")
    section = st.sidebar.radio("é€‰æ‹©å­¦ä¹ æ¨¡å—", [
        "æ–‡æœ¬åˆ†æåŸºç¡€",
        "æ–‡æœ¬é¢„å¤„ç†",
        "æ–‡æœ¬åˆ†ç±»ä¸“é¡¹",
        "æƒ…æ„Ÿåˆ†æä¸“é¡¹",
        "æœ´ç´ è´å¶æ–¯ç®—æ³•",
        "æ¦‚å¿µæµ‹éªŒ",
        "ç¼–ç¨‹å®ä¾‹ï¼ˆæ–°é—»æ–‡æœ¬æ•°æ®é›†ï¼‰"
    ])
    
    # æ˜¾ç¤ºå¯¹åº”æ¨¡å—ç¼–ç¨‹å®ä¾‹æ¨¡å—: è´å¶æ–¯æ–‡æœ¬åˆ†ç±»åˆ†æ­¥ç¼–ç¨‹è®­ç»ƒ"
    st.session_state.section = section    
    context = ""
    if section == "æ–‡æœ¬åˆ†æåŸºç¡€":
        text_introduction_section()
    elif section == "æ–‡æœ¬é¢„å¤„ç†":
        text_preprocessing_section()
    elif section == "æ–‡æœ¬åˆ†ç±»ä¸“é¡¹":
        text_analysis_section()
    elif section == "æƒ…æ„Ÿåˆ†æä¸“é¡¹":
        sentiment_analysis_section()
    elif section == "æœ´ç´ è´å¶æ–¯ç®—æ³•":
        native_bys_section()
    elif section == "æ¦‚å¿µæµ‹éªŒ":
        quiz_section()
    elif section == "ç¼–ç¨‹å®ä¾‹ï¼ˆæ–°é—»æ–‡æœ¬æ•°æ®é›†ï¼‰":
        # åˆå§‹åŒ–stepå˜é‡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'step' not in st.session_state:
            st.session_state.step = 0
        bayes_text_classification_step_by_step.main()
        context = "ç¼–ç¨‹å®ä¾‹æ¨¡å—: æœ´ç´ è´å¶æ–¯æ–‡æœ¬åˆ†ç±»åˆ†æ­¥ç¼–ç¨‹è®­ç»ƒ"

    # æ˜¾ç¤ºèŠå¤©ç•Œé¢
    display_chat_interface(context)
    
    # è®°å½•æ¨¡å—é€€å‡ºæ—¶é—´
    if current_section in st.session_state.text_analysis_records["module_timestamps"]:
        st.session_state.text_analysis_records["module_timestamps"][current_section]["exit_time"] = datetime.now().timestamp()
    
    if section != "ç¼–ç¨‹å®ä¾‹ï¼ˆæ–°é—»æ–‡æœ¬æ•°æ®é›†ï¼‰":
        # ä¾§è¾¹æ æ·»åŠ å­¦ä¹ æŠ¥å‘ŠæŒ‰é’®ï¼ˆè°ƒç”¨ç‹¬ç«‹æ¨¡å—ï¼‰
        st.sidebar.markdown("---")
        if st.sidebar.button("æ–‡æœ¬åˆ†ææ¨¡å—å­¦ä¹ æŠ¥å‘Š"):
            report = generate_evaluation(
                module_type="text_analysis",
                raw_records=st.session_state.text_analysis_records
            )
            st.write("### æ–‡æœ¬åˆ†æå­¦ä¹ æƒ…å†µæŠ¥å‘Š")
            st.info(report)
            
    # ä¾§è¾¹æ ä¿¡æ¯
    st.sidebar.markdown("---")
    st.sidebar.info("""
    æœ¬å¹³å°å¸®åŠ©å­¦ä¹ æ–‡æœ¬åˆ†æåŸºç¡€çŸ¥è¯†ï¼š
    - æ–‡æœ¬é¢„å¤„ç†æ–¹æ³•
    - ç‰¹å¾æå–æŠ€æœ¯ï¼ˆè¯è¢‹æ¨¡å‹ã€TF-IDFï¼‰
    - æ–‡æœ¬åˆ†ç±»ç®—æ³•
    - æƒ…æ„Ÿåˆ†æåŸºç¡€
    """)

if __name__ == "__main__":
    main()









